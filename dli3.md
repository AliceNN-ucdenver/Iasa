# Building AI Agents with Multimodal Models — Complete Reference Table

## NVIDIA Deep Learning Institute Instructor Certification Study Guide

---

| Topic | Details |
|-------|---------|
| **Convolutional Neural Networks (Fundamentals)** | **High-Level Concept:** Convolutional Neural Networks are the foundational architecture for computer vision. They process images by sliding small learnable filters (called kernels) across the image to detect patterns — starting with simple edges and corners in early layers, building up to complex shapes and objects in deeper layers. The key innovation is that the same filter is applied everywhere in the image, so a "cat detector" learned in one corner works in any corner. **Deep Dive:** • A convolution operation takes a small window (typically 3×3 or 5×5 pixels) and computes a weighted sum with a learnable kernel, producing one output value. This slides across the entire image to produce a "feature map" • Pooling layers (typically max pooling) downsample the feature maps by taking the maximum value in each small region, making the network robust to small shifts in position • The "receptive field" is how much of the original image one neuron can "see" — deeper layers have larger receptive fields because they stack on top of earlier layers • In multimodal pipelines, Convolutional Neural Networks serve as the vision encoder that extracts features before fusion with other modalities |
| **Deep Learning Framework (PyTorch)** | **High-Level Concept:** PyTorch is the Python library used to build and train neural networks. It provides tensors (multi-dimensional arrays that can run on Graphics Processing Units), automatic differentiation (computing gradients without manual calculus), and modular building blocks for constructing models. **Deep Dive:** • `torch.nn.Module` is the base class for all neural network components — you define layers in `__init__` and the data flow in `forward()` • Automatic differentiation (autograd) tracks all operations on tensors and can compute gradients by calling `loss.backward()` • The training loop pattern is: forward pass → compute loss → backward pass (gradients) → optimizer step (update weights) → zero gradients • `DataLoader` handles batching, shuffling, and parallel data loading • Moving tensors to Graphics Processing Unit with `.to('cuda')` is essential for fast training |
| **Vision Data Types and Shapes** | **High-Level Concept:** Different sensors capture visual information in different formats. Standard cameras produce dense 2D images with color at every pixel. Depth sensors add distance information. Light Detection and Ranging produces sparse 3D point clouds. Medical scanners produce 3D volumes. Each format has a specific tensor shape that models expect. **Deep Dive:** • Red-Green-Blue images have shape [Batch, 3, Height, Width] — three color channels, each pixel has intensity 0-255 (raw) or 0-1 (normalized) • Grayscale images have shape [Batch, 1, Height, Width] — single intensity channel, computed from Red-Green-Blue as: gray = 0.2126×Red + 0.7152×Green + 0.0722×Blue (weights match human eye sensitivity) • Depth maps have shape [Batch, 1, Height, Width] where each pixel value represents distance in meters rather than color intensity • Point clouds have shape [Number of points, 3 or more] — each row is one 3D point with (x, y, z) coordinates and optionally intensity or color • Computed Tomography scans have shape [Batch, 1, Depth, Height, Width] — the Depth dimension represents spatial slices through the body (not time), requiring 3D convolutions |
| **Audio Spectrograms** | **High-Level Concept:** Raw audio is a one-dimensional waveform (amplitude over time), but neural networks designed for images work on two-dimensional data. Spectrograms transform audio into image-like representations by showing how frequency content changes over time, allowing vision models to process sound. **Deep Dive:** • Raw audio is sampled at a rate like 16,000 samples per second, producing a one-dimensional array of amplitude values • Short-Time Fourier Transform takes small overlapping windows of audio (typically 25 milliseconds) and computes the frequency content in each window, stacking results into a 2D matrix [Frequency bins, Time frames] • The mel scale warps the frequency axis to match human hearing perception — we can easily distinguish 100 Hertz from 200 Hertz, but 10,000 Hertz and 10,100 Hertz sound nearly identical. Formula: mel = 2595 × log₁₀(1 + frequency/700) • Log amplitude converts energy to decibels because human loudness perception is logarithmic • Final output shape is [1, Number of mel bins, Time frames] — this is treated as a single-channel image and can be fed directly to Convolutional Neural Networks or Vision Transformers |
| **Image Color Modes** | **High-Level Concept:** Different software libraries store color channels in different orders. PyTorch and the Python Imaging Library use Red-Green-Blue order. OpenCV uses Blue-Green-Red order. Feeding data in the wrong order causes catastrophic accuracy drops because red objects appear blue and vice versa. **Deep Dive:** • Red-Green-Blue format: channels are ordered [Red, Green, Blue] — this is the standard for most deep learning models and the Python Imaging Library • Blue-Green-Red format: channels are ordered [Blue, Green, Red] — this is the default when loading images with OpenCV's `cv2.imread()` function • If you load an image with OpenCV and feed it directly to a model trained on Red-Green-Blue data, red apples will appear blue, blue sky will appear orange, and accuracy will drop by 50% or more • Always convert explicitly: `image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)` before feeding to models • Red-Green-Blue-Alpha format adds a fourth transparency channel — most models expect only 3 channels, so convert with `image.convert("RGB")` which composites transparency onto white |
| **Computed Tomography Scan Shape** | **High-Level Concept:** Computed Tomography scans are three-dimensional medical images created by taking many X-ray slices through the body. Unlike video where the third dimension is time, in Computed Tomography the third dimension is spatial depth — adjacent slices show adjacent anatomy, so a tumor visible in one slice continues into neighboring slices. **Deep Dive:** • Shape is [Batch, Channels, Depth, Height, Width] — typically [1, 1, 200, 512, 512] meaning 200 slices at 512×512 resolution • Values are in Hounsfield Units calibrated to tissue density: air = -1000, water = 0, soft tissue = +40, bone = +1000, metal implants = +3000 • "Windowing" maps a Hounsfield Unit range to display values — lung window [-1000 to -200] reveals air-filled structures, bone window [-500 to +1500] reveals skeletal structures • You cannot use standard 2D convolutions (Conv2d) because the depth dimension contains important spatial context — a slice above and below a tumor are medically relevant. Use 3D convolutions (Conv3d) or process slices with 2D convolutions then aggregate |
| **Light Detection and Ranging Data** | **High-Level Concept:** Light Detection and Ranging sensors fire laser pulses and measure the time until each pulse returns, calculating precise distances. The output is a "point cloud" — a collection of 3D coordinates representing surfaces the laser hit. Unlike camera images which are dense (every pixel has a value), point clouds are sparse (gaps exist between measured points). **Deep Dive:** • Output shape is [Number of points, 4] where each row contains (x, y, z, intensity) — x/y/z are positions in meters relative to the sensor, intensity is how much laser light reflected back • Typical automotive Light Detection and Ranging produces 100,000 to 1,000,000 points per scan • Coordinate system: X = forward (direction of travel), Y = left, Z = up • Strengths: precise depth to centimeters, works in complete darkness, unaffected by lighting conditions • Weaknesses: sparse (gaps between points), no color or texture information, expensive sensors ($1,000 to $75,000), affected by rain/fog/dust |
| **Camera Data** | **High-Level Concept:** Cameras capture dense two-dimensional images with rich color and texture information at every pixel, but completely lack depth information — you can see a red car clearly but cannot tell if it is 10 meters or 100 meters away without additional cues. **Deep Dive:** • Output shape is [3, Height, Width] for Red-Green-Blue images — every pixel has color information • Strengths: rich semantic information (color, texture, text, patterns), dense coverage, inexpensive sensors ($50-500), well-understood processing pipelines • Weaknesses: no direct depth measurement (depth must be inferred from stereo or motion), fails in darkness without illumination, affected by glare and reflections • Coordinate system: X = right (image columns), Y = down (image rows), Z = forward (into the scene) — note this differs from Light Detection and Ranging coordinates |
| **Light Detection and Ranging to Camera Projection** | **High-Level Concept:** To combine Light Detection and Ranging points with camera images, you must project the 3D points onto the 2D image plane. This requires two transformations: first, move points from the Light Detection and Ranging coordinate system to the camera coordinate system (using the physical mounting positions of the sensors); second, project from 3D camera coordinates to 2D pixel positions (using the camera's optical properties). **Deep Dive:** • Step 1 — Extrinsic transformation: The Light Detection and Ranging and camera are mounted at different positions and angles on the vehicle. The rotation matrix R (3×3) accounts for angle differences, the translation vector t (3×1) accounts for position offset. Formula: Point_camera = R × Point_lidar + t • Step 2 — Intrinsic projection: The camera intrinsic matrix K (3×3) contains focal lengths (how much the lens zooms) and principal point (where the optical axis hits the sensor). Formula: pixel_coordinates = K × Point_camera / Z, where dividing by Z (depth) creates perspective — objects appear smaller when farther away • The division by Z is the key mathematical operation — it is why parallel railroad tracks appear to converge at the horizon, why distant mountains look smaller than nearby trees. This is the same perspective projection Renaissance painters discovered • After projection, Light Detection and Ranging points appear sparse on the image — there are far fewer laser returns than camera pixels, creating "gaps" in the projected depth information |
| **Early Fusion (Input-Level Combination)** | **High-Level Concept:** Early fusion combines multiple modalities at the input level by concatenating them into a single tensor before feeding to the model. For Red-Green-Blue images and depth maps, this means creating a 4-channel input instead of 3-channel, so the model processes both modalities through the same network from the very first layer. **Deep Dive:** • How it works with Red-Green-Blue + Depth: Take the 3-channel color image [Batch, 3, Height, Width] and 1-channel depth map [Batch, 1, Height, Width], concatenate along channel dimension to get [Batch, 4, Height, Width] • In the notebook: We modified the first convolutional layer of VGG to accept 4 input channels instead of 3. The code changes `model.features[0] = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=3, padding=1)`. The new channel weights must be initialized (typically copied from an existing channel or randomly initialized) • The model then processes this 4-channel input through all layers, learning to extract features that combine color and depth information from the very first convolution • Advantages: The model can learn subtle cross-modal correlations from the start; no separate processing branches needed • Disadvantages: Requires pixel-aligned inputs (depth and color must match exactly); cannot use pretrained models directly (they expect 3 channels); if one modality is noisy or missing, it affects all processing |
| **Late Fusion (Decision-Level Combination)** | **High-Level Concept:** Late fusion runs completely separate models on each modality, then combines their predictions at the very end. Each modality gets its own specialized encoder, and the final classification is made by averaging, voting, or learning to weight the separate predictions. **Deep Dive:** • How it works: The Red-Green-Blue image goes through one complete Convolutional Neural Network producing prediction probabilities [Batch, Number of classes]. The depth map goes through a separate Convolutional Neural Network producing its own probabilities [Batch, Number of classes]. These predictions are then combined • In the notebook: We ran two separate forward passes — `logits_rgb = model_rgb(rgb_image)` and `logits_depth = model_depth(depth_image)`. Then combined with simple averaging: `final_logits = (logits_rgb + logits_depth) / 2`. Alternative combinations include learned weights, concatenation followed by a small network, or voting schemes • The key insight is that the models never see each other's raw data — the Red-Green-Blue model has no idea about depth, the depth model has no idea about color. They only interact at the final prediction stage • Advantages: Can use best pretrained model for each modality; robust when one modality is missing (just use the other); simple and modular architecture • Disadvantages: No cross-modal reasoning during feature extraction; "cat in image" and "cat in depth" never interact until the very end; may miss correlations that require seeing both modalities together |
| **Intermediate Fusion (Feature-Level Combination)** | **High-Level Concept:** Intermediate fusion uses separate encoders for each modality but combines their learned representations partway through the network, allowing cross-modal interaction during feature learning while still leveraging modality-specific pretrained encoders. **Deep Dive:** • How it works: Each modality goes through its own encoder (Convolutional Neural Network or Vision Transformer) producing feature vectors. These feature vectors are then combined — typically by concatenation, addition, or cross-attention — and passed through additional shared layers before the final prediction • The combination happens at a "feature level" — not raw pixels (early) and not final predictions (late), but somewhere in between where each encoder has already extracted meaningful representations • Cross-attention mechanism: One modality's features become "queries" that attend to the other modality's features (keys and values). Formula: Attention = softmax(Queries × Keys^T / √dimension) × Values. This allows, for example, text tokens to "look at" relevant parts of an image • Projection at the fusion layer: When combining features from different encoders, dimensions often differ (e.g., image features are 1024-dimensional, text features are 768-dimensional). A learned projection layer (linear transformation or multi-layer perceptron) maps them to a common dimension before fusion • Advantages: Best performance in most benchmarks; enables deep cross-modal reasoning; attention weights are interpretable • Disadvantages: More complex architecture; requires careful design of the fusion mechanism |
| **Contrastive Language-Image Pre-training Architecture** | **High-Level Concept:** Contrastive Language-Image Pre-training (often called CLIP) is a dual-encoder model that learns to map images and text into the same embedding space, where matching image-text pairs are close together and non-matching pairs are far apart. Critically, there is no cross-attention between modalities during encoding — the image encoder never sees text, and the text encoder never sees images. They only interact through similarity comparison of their final embeddings. **Deep Dive:** • Architecture: Two completely separate encoders — a Vision Transformer (or Convolutional Neural Network) for images producing a 512-dimensional embedding, and a Transformer for text producing a 512-dimensional embedding. Each embedding is L2-normalized to unit length • Why separate encoders matter: Because encoders don't interact, you can pre-compute image embeddings offline. Store embeddings for a million images, then search with any new text query without re-running the image encoder. If cross-attention existed, you would need to run both encoders together for every query • Training data: 400 million image-text pairs scraped from the internet (alt text, captions). No manual labeling required — the pairing itself provides supervision • Zero-shot classification: To classify an image into categories without any training, encode text prompts like "a photo of a dog" and "a photo of a cat", compute similarity with the image embedding, and pick the highest similarity. The model has never been explicitly trained on these categories |
| **Contrastive Training (Learning by Comparison)** | **High-Level Concept:** Contrastive training teaches a model to distinguish matching pairs from non-matching pairs. Given a batch of image-text pairs, the model learns to pull matching pairs together in embedding space (make their vectors similar) and push non-matching pairs apart (make their vectors dissimilar). The batch structure provides automatic labeling — no human annotation needed. **Deep Dive:** • How pulling and pushing works: Consider a batch of 4 image-text pairs: (image₀, text₀), (image₁, text₁), (image₂, text₂), (image₃, text₃). Image₀ should be similar to text₀ (pull together) but dissimilar to text₁, text₂, text₃ (push apart). The loss function penalizes high similarity with wrong texts and rewards high similarity with the correct text • The similarity matrix: Compute embeddings for all images [4, 512] and all texts [4, 512]. Matrix multiply to get all pairwise similarities [4, 4]. The diagonal entries (i, i) are the matching pairs that should have high values. Off-diagonal entries (i, j where i≠j) are non-matching pairs that should have low values • The cross-entropy trick: Treat each row of the similarity matrix as a probability distribution over texts. The correct text for image_i is text_i (index i). So labels = [0, 1, 2, 3]. Cross-entropy loss pushes probability mass toward the correct index • Symmetric loss: Compute loss both ways — image-to-text (which text matches each image?) and text-to-image (which image matches each text?). Average the two losses. This ensures both encoders are trained equally • Why batch size matters: With batch size 32, each image has 1 correct text and 31 wrong texts — a relatively easy task. With batch size 32,768 (what Contrastive Language-Image Pre-training used), each image must be distinguished from 32,767 wrong texts — forcing the model to learn very fine-grained features |
| **Cosine Similarity** | **High-Level Concept:** Cosine similarity measures how aligned two vectors are by computing the cosine of the angle between them, ignoring their magnitudes. Two vectors pointing in the same direction have cosine similarity of 1, perpendicular vectors have 0, and opposite vectors have -1. This is the standard metric for comparing embeddings because semantic similarity should not depend on arbitrary vector lengths. **Deep Dive:** • Formula: cosine_similarity(A, B) = (A · B) / (||A|| × ||B||) = sum of element-wise products divided by the product of the vector lengths • Range: [-1, 1] — unlike dot product which is unbounded • Why not just dot product? Dot product is affected by magnitude: a very long vector will have high dot product with everything. Two semantically similar concepts might have low dot product just because one embedding happens to be shorter. Cosine similarity removes this confound • Practical implementation: L2-normalize all embeddings to unit length first (divide each vector by its length). After normalization, dot product equals cosine similarity, so you can use fast matrix multiplication: `similarity_matrix = normalized_embeddings @ normalized_embeddings.T` |
| **Ground Truth Labels in Contrastive Learning** | **High-Level Concept:** In contrastive learning, labels are not manually assigned categories like "dog" or "cat". Instead, labels are simply the indices of matching pairs within the batch. If image₀ matches text₀, image₁ matches text₁, and so on, then the labels are [0, 1, 2, ...]. The batch structure itself provides supervision. **Deep Dive:** • Why labels = indices: When you compute cross-entropy loss on the similarity matrix, you are asking "which text is correct for image_i?" The answer is "text_i" — the text at the same index. So label[i] = i • Code implementation: `labels = torch.arange(batch_size, device=device)` creates [0, 1, 2, ..., batch_size-1] • This is self-supervised learning: No human ever labeled these images. The supervision comes from how data was collected — each image was paired with its caption on a webpage. The model learns that image_i should match text_i simply because they appeared together • The elegance: This scales to 400 million pairs without any manual annotation cost |
| **Information Noise Contrastive Estimation Loss (Contrastive Loss Function)** | **High-Level Concept:** The loss function for contrastive learning is symmetric cross-entropy over the similarity matrix, often called Information Noise Contrastive Estimation. It treats the task as classification: "Given image_i, classify which of the N texts is correct." Temperature scaling controls how "peaked" or "spread out" the probability distribution is. **Deep Dive:** • Step-by-step computation: (1) Encode images and texts into embeddings. (2) L2-normalize embeddings. (3) Compute similarity matrix via matrix multiplication [N × N]. (4) Multiply by temperature (or divide, depending on convention). (5) Apply cross-entropy loss with labels = [0, 1, ..., N-1] • Temperature parameter (τ): Controls sharpness of the softmax distribution. Low temperature (0.07, what Contrastive Language-Image Pre-training uses) makes the distribution very peaked — the model is very confident. High temperature (1.0) makes the distribution softer. Temperature is typically learned during training • Symmetric loss: `loss_image_to_text = cross_entropy(similarity_matrix, labels)` asks "for each image, which text?" `loss_text_to_image = cross_entropy(similarity_matrix.T, labels)` asks "for each text, which image?" Final loss = average of both • The loss decreases when: matching pairs have high similarity (diagonal of the matrix) and non-matching pairs have low similarity (off-diagonal) |
| **repeat_interleave versus repeat (PyTorch Tensor Operations)** | **High-Level Concept:** These two PyTorch operations both duplicate tensor elements but in different patterns. `repeat_interleave` repeats each element in place (like stuttering: A-A-A-B-B-B-C-C-C). `repeat` tiles the entire tensor (like echoing: A-B-C-A-B-C-A-B-C). Using the wrong one scrambles your data alignment in contrastive learning. **Deep Dive:** • `tensor.repeat_interleave(n)`: Each element repeats n times before moving to the next. Input [A, B, C] with n=3 becomes [A, A, A, B, B, B, C, C, C]. Use when you want each item to match against multiple other items • `tensor.repeat(n)`: The entire tensor is tiled n times. Input [A, B, C] with n=3 becomes [A, B, C, A, B, C, A, B, C]. Use when you want multiple copies of the whole sequence • When to use each in contrastive learning: If you have 4 images and want each image to compare against 3 texts, use `repeat_interleave(3)` on images → [I₀,I₀,I₀, I₁,I₁,I₁, I₂,I₂,I₂, I₃,I₃,I₃] and `repeat(4)` on texts → [T₀,T₁,T₂, T₀,T₁,T₂, T₀,T₁,T₂, T₀,T₁,T₂]. Now position k of images aligns with position k of texts correctly |
| **Vector Database** | **High-Level Concept:** A vector database stores embedding vectors and enables fast similarity search — given a query vector, find the most similar vectors among millions of stored entries. This is essential for retrieval systems where comparing to every vector (brute force) would be too slow. **Deep Dive:** • The problem: You have 1 million document embeddings (768 dimensions each). A user query needs the 10 most similar. Brute force requires 1 million dot products per query — too slow for real-time applications • Hierarchical Navigable Small World (HNSW) solution: Build a multi-layer graph during indexing. Top layers are sparse (few nodes, coarse navigation), bottom layers are dense (many nodes, precise). Search starts at the top, greedily moves toward the query, drops to the next layer, continues until reaching the bottom layer for final refinement. Complexity: O(log N) instead of O(N) • Key parameters: M = number of edges per node (more edges = more accurate but more memory), ef_construction = search depth during index building, ef_search = search depth during queries (higher = more accurate but slower) • Popular vector databases: Milvus (open source, distributed), Pinecone (managed service), Weaviate (hybrid search), Qdrant (Rust, very fast), pgvector (PostgreSQL extension) |
| **Retrieval Augmented Generation** | **High-Level Concept:** Retrieval Augmented Generation grounds Large Language Model responses in external documents by first retrieving relevant text chunks, then including them in the prompt. This reduces hallucination (making things up), enables access to private or recent information not in training data, and allows citing sources. **Deep Dive:** • Indexing phase (offline): Split documents into chunks (typically 200-500 tokens), embed each chunk using an embedding model, store embeddings in a vector database with metadata (source, page number, etc.) • Retrieval phase (online): When a user asks a question, embed the question using the same embedding model, search the vector database for top-k most similar chunks • Augmentation phase: Insert retrieved chunks into the prompt as "context" before the user's question. Template: "Based on the following context, answer the question. Context: {chunk1} {chunk2} {chunk3}. Question: {user_query}" • Generation phase: The Large Language Model generates an answer grounded in the retrieved context rather than relying solely on its training data • Why it helps: Without retrieval, the model only knows what was in its training data (with a knowledge cutoff date) and may hallucinate. With retrieval, it can access up-to-date, private, or specialized documents and cite specific sources |
| **Vision Language Models** | **High-Level Concept:** Vision Language Models accept both images and text as input and generate text output with visual grounding. They work by converting image features into "visual tokens" that the Large Language Model can process alongside text tokens, allowing the model to describe images, answer questions about them, or follow visual instructions. **Deep Dive:** • Core architecture: Three components — (1) Vision Encoder: a pretrained Convolutional Neural Network or Vision Transformer that extracts image features, (2) Projection Layer: transforms vision features to match the Large Language Model's embedding dimension, (3) Large Language Model: processes the combined visual and text tokens to generate output • The key insight: Large Language Models process sequences of tokens. Images are not tokens, but we can project image features into the same dimensional space as text embeddings. The Large Language Model then treats these "visual tokens" like words in a foreign language it has learned to understand • Input sequence structure: [visual_token_1, visual_token_2, ..., visual_token_256, User, :, What, is, in, this, image, ?]. The visual tokens come from projecting the vision encoder's output (typically 256 patches from a Vision Transformer) |
| **Cross-Modal Projection** | **High-Level Concept:** Cross-modal projection is the learned transformation that maps vision encoder outputs into the Large Language Model's embedding space. It solves two problems: dimension mismatch (vision features might be 1024-dimensional while the Large Language Model expects 4096-dimensional) and semantic mismatch (vision features encode visual patterns, while the Large Language Model expects word-like meanings). **Deep Dive:** • Why dimension alignment is not enough: A simple linear layer can change dimensions (1024 → 4096), but vision and language embedding spaces have different structures. Concepts that are close in vision space (two different dog photos) might not be close in language space (where "dog" is near "pet" and "animal") • Multi-layer perceptron projection: Use two linear layers with a nonlinear activation (GELU) in between. This allows the transformation to "warp" the space — bending, folding, and stretching to map one topology onto another. Single linear layers can only rotate, scale, and translate, preserving straight lines • Training the projector: Minimize distance between projected visual features and corresponding text embeddings. When the projector sees visual features of a dog, it should output something close to where the Large Language Model represents the word "dog" • In Large Language and Vision Assistant (LLaVA): The projection is a 2-layer multi-layer perceptron with approximately 768,000 trainable parameters — tiny compared to the frozen vision encoder and Large Language Model |
| **Large Language and Vision Assistant Architecture** | **High-Level Concept:** Large Language and Vision Assistant (LLaVA) is a reference architecture for Vision Language Models consisting of a frozen Contrastive Language-Image Pre-training vision encoder, a trainable projection layer, and a Vicuna Large Language Model. It uses efficient two-stage training: first align features by training only the projector, then fine-tune for instruction following. **Deep Dive:** • Architecture components: (1) Vision Encoder: Contrastive Language-Image Pre-training Vision Transformer-Large with 14×14 patches, producing 257 tokens of 1024 dimensions (256 patches + 1 classification token), kept frozen throughout training. (2) Projection: 2-layer multi-layer perceptron mapping 1024 → 4096 dimensions, the only component trained in stage 1. (3) Large Language Model: Vicuna-7B or 13B (LLaMA fine-tuned on conversation data), frozen in stage 1, fine-tuned with Low-Rank Adaptation in stage 2 • Stage 1 — Feature Alignment: Train only the projection layer on 558,000 image-caption pairs. Goal: teach the projector to "speak" the Large Language Model's language. Compute: approximately 4 hours on 8 A100 Graphics Processing Units • Stage 2 — Visual Instruction Tuning: Train projection + Large Language Model (with Low-Rank Adaptation) on 158,000 instruction-following conversations. Goal: teach the combined system to follow complex visual instructions • Why two stages? Stage 1 alone can describe images but does not follow instructions well. Stage 2 alone is unstable because the projector is randomly initialized. Both stages together: projector learns basics first, then the full system is fine-tuned for high quality |
| **Portable Document Format Chunking Strategies** | **High-Level Concept:** Before documents can be searched with Retrieval Augmented Generation, they must be split into smaller chunks. Chunking strategy dramatically affects retrieval quality — chunks too large dilute relevance, chunks too small lose context. Different strategies include fixed-size, recursive, semantic, and layout-aware chunking. **Deep Dive:** • Fixed-size chunking: Split every N characters (e.g., 1000 characters with 200 character overlap). Simple but may split mid-sentence: "The CEO stated that profitability" | "will increase by 40%" — meaning is broken • Recursive chunking: Try to split by paragraph first, then by sentence, then by word, using the largest unit that keeps chunks under the size limit. Better preserves meaning than fixed-size • Semantic chunking: Embed each sentence, compute similarity between adjacent sentences, split where similarity drops significantly (indicating topic change). Most sophisticated text-only approach • Layout-aware chunking: Use document structure — keep tables intact as single chunks, group headers with their following paragraphs, preserve list items together, keep figures with their captions. Best for structured documents with mixed content types • Chunk size guidelines: Most embedding models have a 512-token context window. For Retrieval Augmented Generation with Large Language Models having 2048-8192 token context, use 200-500 token chunks with 50-100 token overlap |
| **Page Element Identification** | **High-Level Concept:** Before processing a document, identify what type of content is in each region: titles, body paragraphs, tables, figures, lists, headers, footers. This enables appropriate handling — tables should be kept whole, figures need image processing, body text can be chunked normally. **Deep Dive:** • Element types to detect: Title (document or section headers), NarrativeText (body paragraphs), ListItem (bulleted or numbered), Table (structured rows and columns), Figure (images, charts, diagrams), Caption (figure or table descriptions), Header/Footer (page metadata), PageNumber, Formula (mathematical equations) • Detection pipeline: (1) Layout model (YOLOX, Detectron2, or LayoutLM) identifies bounding boxes and element types, (2) Optical Character Recognition extracts text from each region, (3) Reading order is determined by sorting regions by position • Special handling by element type: Tables should be kept as complete units (not split mid-row), converted to markdown or described in natural language. Figures need image understanding (Vision Language Model captioning) or at minimum their captions. Lists should preserve item structure • Tools: `unstructured` library (open source Python), LayoutLM/LayoutLMv3 (Microsoft, state of the art), DocTR (Optical Character Recognition + layout), NVIDIA Data Loading Library (GPU-accelerated) |
| **Video Search and Summarization Applications** | **High-Level Concept:** Video Search and Summarization enables natural language queries over large video archives, returning timestamped results and summaries. Instead of humans watching hundreds of hours of footage, the system finds relevant moments in seconds. Applications span surveillance, media production, sports analytics, education, and compliance. **Deep Dive:** • Surveillance and safety: "Show all Personal Protective Equipment violations this week" — system finds all instances of workers without hard hats, returns timestamps and descriptions. Saves security teams from watching 24/7 footage • Media production: "Find all clips of the CEO mentioning revenue growth" — enables journalists and editors to locate specific moments across interviews, press conferences, earnings calls without manual review • Sports analytics: "Show all three-point shots in the fourth quarter" — coaches can study specific play patterns without watching entire games. Also used for highlight generation • Compliance and legal: "Find all conversations where pricing was discussed" — supports audits, legal discovery, regulatory compliance by making video archives searchable • Value proposition: A human reviewing 100 hours of video takes days of work. Video Search and Summarization answers queries in seconds, with humans reviewing only the returned clips for verification |
| **NVIDIA AI Blueprint for Video Search and Summarization** | **High-Level Concept:** The NVIDIA AI Blueprint provides a complete reference architecture for Video Search and Summarization, integrating video processing, computer vision, vision-language models, Large Language Models, retrieval systems, and databases into a production-ready pipeline. Each component is packaged as an NVIDIA Inference Microservice for optimized deployment. **Deep Dive:** • Data Plane layer: DeepStream Software Development Kit handles video decoding and frame extraction. Videos are chunked into segments (typically 10-60 seconds). Keyframes are sampled from each chunk for analysis • Computer Vision layer: Object detection identifies people, vehicles, objects in frames. Tracking maintains identity across frames (person_1 stays person_1). Segmentation provides pixel-level scene understanding • Vision Language Model layer: Frame captioning generates natural language descriptions of what is happening. Contrastive Language-Image Pre-training embedding creates searchable vectors for each chunk. Optical Character Recognition extracts any text visible in frames • Storage layer: Vector database (Milvus) stores embeddings for semantic similarity search. Graph database (Neo4j) stores entity relationships for multi-hop reasoning. Both are queried during retrieval • NeMo Retriever layer: Handles the full retrieval pipeline — embedding queries, searching databases, re-ranking results, assembling context. Supports vector-only, graph-only, or hybrid retrieval • Large Language Model layer: Given retrieved video chunk descriptions, generates natural language answers to user queries, produces summaries, answers follow-up questions • NVIDIA Inference Microservice: Each component is a containerized, GPU-optimized microservice with a REST API. Pre-built for common tasks, easy to deploy and scale |
| **Video Search and Summarization Architecture (Implementation Details)** | **High-Level Concept:** The technical implementation of Video Search and Summarization follows a pipeline: video → chunks → keyframes → embeddings → storage → retrieval → summarization. Each stage has design choices affecting latency, quality, and cost. **Deep Dive:** • Stage 1 — Video ingestion: Split videos into chunks (10-60 seconds). Shorter chunks enable more precise retrieval but require more storage. 15-30 seconds is typical for most applications • Stage 2 — Keyframe sampling: Extract representative frames from each chunk. Options: 1 frame per chunk (fast but may miss events), 5 frames per chunk (balanced), every frame (expensive, diminishing returns) • Stage 3 — Embedding: Encode keyframes using Contrastive Language-Image Pre-training vision encoder. Options: average frame embeddings (simple but loses temporal information), concatenate (preserves order but larger vectors), temporal pooling (best but complex) • Stage 4 — Captioning: Generate natural language description of each chunk using a Vision Language Model. "A forklift moves through a warehouse. A worker in a yellow vest walks past." This caption enables text-based search even when visual search misses • Stage 5 — Indexing: Store embeddings in vector database with metadata (video ID, start time, end time, caption, camera ID). Create Hierarchical Navigable Small World index for fast search • Stage 6 — Query processing: Embed user query using same Contrastive Language-Image Pre-training text encoder. Search vector database for similar chunks. Optionally apply metadata filters (camera, date, time) • Stage 7 — Response generation: Pass retrieved chunk captions to Large Language Model with user query. Generate natural language summary with timestamps |
| **Vector-Based Retrieval Augmented Generation** | **High-Level Concept:** Vector-based Retrieval Augmented Generation finds relevant content by computing semantic similarity between the query embedding and stored chunk embeddings. It excels at finding conceptually similar content regardless of exact word matches — "safety incidents" will match "near-miss events" even without shared keywords. **Deep Dive:** • How it works: (1) Embed user query using same model used for chunks. (2) Search vector database for top-k chunks with highest cosine similarity to query embedding. (3) Return chunks ranked by similarity score • Advantages: Semantic matching finds conceptually related content even with different wording. "What safety incidents occurred?" matches "worker slipped near forklift" and "near-miss with loading equipment" despite no exact keyword overlap. Works across languages with multilingual embeddings • Limitations: May miss exact keyword matches — "Show clip 47" might return visually similar clips instead of the exact clip numbered 47. No relational reasoning — "Who talked to the person in the red shirt?" requires tracking entities across frames, not just similarity. No temporal reasoning — "What happened after the alarm?" requires understanding time sequence, not just content similarity |
| **Graph-Based Retrieval Augmented Generation** | **High-Level Concept:** Graph-based Retrieval Augmented Generation uses knowledge graphs to answer questions requiring relationship traversal. Instead of finding similar content, it follows edges between entities to answer multi-hop questions like "Which companies has the CEO of Acme Corporation previously worked for?" **Deep Dive:** • Knowledge graph structure: Nodes represent entities (people, companies, locations, events, objects). Edges represent relationships (works_at, located_in, acquired_by, interacted_with). Properties store attributes (name, timestamp, color) • Why graphs solve problems vectors cannot: Query "Who talked to the person in red?" requires: (1) identify "person in red" as an entity, (2) find "talked_to" relationships involving that entity, (3) return the other participants. Vector similarity cannot chain these relationship steps • Building the graph: Extract entities from video captions using Named Entity Recognition. Extract relationships using relation extraction models. Create nodes and edges: (Person: "John Smith") -[:ENTERS]-> (Location: "Warehouse") -[:AT_TIME]-> (Timestamp: "2:00 PM") • Graph queries: "Find all people who interacted with the forklift" becomes a graph traversal: MATCH (p:Person)-[:INTERACTED_WITH]->(f:Object {type: "forklift"}) RETURN p. Multi-hop: "Find objects in the same location as the safety violation" traverses through location nodes • Hybrid approach: Combine vector search (find relevant chunks) with graph traversal (follow relationships) for best results |
| **Context-Aware Retrieval Augmented Generation** | **High-Level Concept:** Context-aware Retrieval Augmented Generation improves precision by filtering results based on metadata before or after vector search, and enriching context by including surrounding content. Instead of searching all videos for "safety incidents," it filters to "Camera 3, yesterday" first, then searches within that subset. **Deep Dive:** • Metadata filtering: Extract structured constraints from user query. "Show safety incidents from Camera 3 yesterday" → filters: {camera: "3", date: "2024-01-15", event_type: "safety_incident"}. Apply these filters to the vector database before computing similarity, dramatically reducing the search space • Filter types: Temporal (date, time range, "last 7 days"), Spatial (camera ID, location name, zone), Categorical (event type, severity level, status) • Context expansion: After retrieving matching chunks, also fetch adjacent chunks (the segment before and after each match). This provides temporal context — what led up to the event and what happened afterward • Re-ranking: Initial vector search returns candidates ranked by embedding similarity. A re-ranker model (cross-encoder) scores each candidate with the full query for more accurate final ranking. More expensive but more precise • Router pattern: Use a Large Language Model to parse user queries and extract filters. Input: "Show me what happened at the loading dock around 2 PM on Tuesday". Output: JSON {location: "loading_dock", time_range: ["13:30", "14:30"], date: "2024-01-16"} |
| **Cypher Query Language** | **High-Level Concept:** Cypher is the declarative query language for Neo4j and other graph databases. It uses visual ASCII-art-like syntax to express graph patterns — nodes in parentheses, relationships in brackets with arrows. Essential for Graph-based Retrieval Augmented Generation to traverse knowledge graphs. **Deep Dive:** • Basic syntax: Nodes are `(variable:Label {property: value})`. Relationships are `-[variable:TYPE]->` (directed) or `-[variable:TYPE]-` (either direction). Full pattern: `MATCH (p:Person)-[r:WORKS_AT]->(c:Company) WHERE c.name = "Acme" RETURN p.name` • Core commands: MATCH finds patterns in the graph. WHERE filters results. RETURN specifies what to output. CREATE adds new nodes/edges. MERGE creates only if not existing • Multi-hop queries: Find friends-of-friends: `MATCH (me:Person {name: "Alice"})-[:KNOWS]->()-[:KNOWS]->(fof) RETURN DISTINCT fof.name`. Variable-length paths: `MATCH (start)-[:CONNECTED*1..3]-(end)` finds paths 1 to 3 hops long • Aggregation: Count relationships: `MATCH (p:Person)-[:WORKED_AT]->(c:Company) RETURN p.name, count(c) as num_companies`. Collect into list: `RETURN p.name, collect(friend.name) as friends` • Large Language Model-generated Cypher: Natural language queries can be converted to Cypher by a Large Language Model. "Who works at Acme?" → `MATCH (p:Person)-[:WORKS_AT]->(c:Company {name:"Acme"}) RETURN p.name` |
| **NeMo Retriever** | **High-Level Concept:** NeMo Retriever is NVIDIA's retrieval microservice that handles the full Retrieval Augmented Generation pipeline — document ingestion, embedding, indexing, query processing, and result ranking. It supports vector search, graph search, and hybrid approaches, providing a unified interface for retrieval across different backends. **Deep Dive:** • Ingestion pipeline: Accepts documents (text, Portable Document Format, HTML). Chunks documents using configurable strategies. Embeds chunks using state-of-the-art embedding models (E5, BGE, NV-Embed). Stores in vector database with metadata • Indexing: Creates Hierarchical Navigable Small World indexes for fast approximate nearest neighbor search. Supports multiple vector databases (Milvus, Pinecone, Weaviate). Handles incremental updates without full re-indexing • Query processing: Embeds incoming queries. Applies metadata filters before search. Retrieves top-k candidates. Optionally re-ranks using cross-encoder models for higher precision • Hybrid retrieval: Combines vector similarity search with graph traversal. Vector search finds semantically similar content. Graph traversal follows entity relationships. Results are merged and re-ranked • Integration with Large Language Model: Retrieved context is formatted and passed to Large Language Model for answer generation. Supports streaming responses. Tracks which chunks were used for citation |

---

## Certification Quick Reference Checklist

| Topic | Key Points to Explain |
|-------|----------------------|
| Convolutional Neural Networks | Kernels slide across image detecting features; pooling reduces dimensions; receptive field grows with depth |
| Vision data shapes | Red-Green-Blue [Batch,3,H,W], Grayscale [Batch,1,H,W], Computed Tomography [Batch,1,Depth,H,W], Point Cloud [N,3+] |
| Audio spectrograms | Short-Time Fourier Transform → mel scale → log amplitude → treat as image |
| Color modes | Red-Green-Blue versus Blue-Green-Red ordering; OpenCV default is Blue-Green-Red; must convert |
| Light Detection and Ranging projection | Extrinsic (rotation + translation) → Intrinsic (focal length, principal point) → divide by depth for perspective |
| Early fusion | Concatenate channels [3+1=4 channels], modify first conv layer, single network processes both |
| Late fusion | Separate models, separate predictions, combine at end (average or learned weights) |
| Intermediate fusion | Separate encoders, project to common dimension, cross-attention or concatenation, shared layers |
| Contrastive Language-Image Pre-training | Dual encoder, NO cross-attention, similarity via dot product, enables offline precomputation |
| Contrastive training | Pull matching pairs together (high similarity), push non-matching apart (low similarity) |
| Ground truth labels | labels = indices because image_i matches text_i; self-supervised from batch structure |
| Contrastive loss | Symmetric cross-entropy on similarity matrix; temperature controls sharpness |
| repeat_interleave vs repeat | Interleave stutters each element; repeat tiles entire tensor |
| Vector database | Hierarchical Navigable Small World enables O(log N) approximate nearest neighbor search |
| Retrieval Augmented Generation | Index → retrieve → augment prompt → generate grounded answer |
| Vision Language Models | Vision encoder → projection → Large Language Model; visual tokens processed like text |
| Cross-modal projection | Multi-layer perceptron warps vision space to language space; handles dimension + semantic mismatch |
| Large Language and Vision Assistant | Two-stage training: (1) align projector only, (2) fine-tune with instruction data |
| Document chunking | Fixed, recursive, semantic, layout-aware; 200-500 tokens with overlap |
| Page elements | Titles, paragraphs, tables, figures, lists; layout models detect regions |
| Video Search and Summarization | Natural language queries over video archives; returns timestamps + summaries |
| NVIDIA Blueprint | Data plane → Computer Vision → Vision Language Model → storage → retrieval → Large Language Model |
| Vector versus Graph Retrieval | Vector for semantic similarity; Graph for relationship traversal |
| Context-aware Retrieval | Metadata filtering before search; context expansion after; re-ranking for precision |
| Cypher queries | MATCH patterns, WHERE filters, RETURN results; multi-hop with variable-length paths |
| NeMo Retriever | Ingestion → indexing → query → re-ranking; vector + graph hybrid retrieval |

---

*End of Complete Reference Table*
