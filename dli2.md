# ğŸ”¥ MASTER CERTIFICATION TABLE â€” Building AI Agents with Multimodal Models

## Complete Deep-Dive Study Guide for NVIDIA DLI Instructor Certification

---

# Part 1 â€” Multimodal Data

| Task | Details | Deep Dive: Architecture, Math, Code & Theory | Notebook / Lab Context |
|------|---------|---------------------------------------------|------------------------|
| **Multimodal Concept** | Multimodal AI combines different signal types (vision, audio, text, tabular, etc.) so models can reason using multiple "senses." **Deep dive â†’** Each human sense maps to one modality: vision = images/video, audition = audio/speech, language = text. Multimodal systems combine these to overcome single-modality limitations. **Key insight â†’** No single modality captures complete reality â€” cameras miss depth, LiDAR misses color, text misses visual nuance. | **Why multimodal matters:**<br>â€¢ Single modality = partial view of reality<br>â€¢ Camera: rich texture/color, no depth, fails in darkness<br>â€¢ LiDAR: precise depth, sparse, no color/texture<br>â€¢ Text: abstract concepts, no visual grounding<br>â€¢ Audio: temporal patterns, no spatial info<br><br>**Modality alignment challenges:**<br>â€¢ Temporal sync (audio-video lip sync)<br>â€¢ Spatial alignment (LiDAR-camera calibration)<br>â€¢ Semantic alignment (image-caption correspondence)<br><br>**Common architectures:**<br>â€¢ Dual encoder (CLIP): separate encoders, shared space<br>â€¢ Cross-attention (Flamingo): one modality attends to other<br>â€¢ Early fusion (RGB-D): concatenate inputs | Lab 01a, 01b explore different modalities. Key takeaway: understand shape conventions [B,C,H,W] for vision, [B,T] for audio waveforms, [B,N,D] for sequences. |
| **Vision Data Types** | **Deep dive â†’** RGB images [3,H,W], grayscale [1,H,W], depth maps [1,H,W] with distance values, point clouds [N,3+] sparse 3D coordinates. Each requires different preprocessing and model architectures. | **RGB Images:**<br>â€¢ Shape: `[Batch, 3, Height, Width]`<br>â€¢ Values: 0-255 raw â†’ normalize to 0-1 or ImageNet stats<br>â€¢ `mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]`<br><br>**Grayscale:**<br>â€¢ Shape: `[B, 1, H, W]`<br>â€¢ Conversion: `gray = 0.2126*R + 0.7152*G + 0.0722*B`<br>â€¢ Why these weights? Matches human luminance perception (more green cones)<br><br>**Depth Maps:**<br>â€¢ Shape: `[B, 1, H, W]` â€” pixel value = distance in meters<br>â€¢ Dense (stereo cameras) vs sparse (projected LiDAR)<br>â€¢ Often normalized: `depth_norm = (depth - min) / (max - min)`<br><br>**Point Clouds:**<br>â€¢ Shape: `[N, 3]` minimum (x,y,z) or `[N, 4+]` with intensity/color<br>â€¢ Sparse, unordered â€” can't use Conv2d directly<br>â€¢ Processing: PointNet, voxelization, or projection to 2D | Lab 01b: Load and visualize each type. Use `PIL` for images, `Open3D` for point clouds. Verify shapes match expected dimensions before feeding to models. |
| **Audio Spectrogram** | Audio is transformed into a 2D image-like representation via Short-Time Fourier Transform (STFT) so vision models can process it. **Deep dive â†’** Raw audio = 1D waveform [T samples]. STFT windows the signal, applies FFT per window, stacks results into [Freq, Time] matrix. Mel scale warps frequency to match human perception. | **Complete Pipeline:**<br><br>**Step 1 â€” Raw Audio:**<br>â€¢ Shape: `[T]` where T = samples (e.g., 16000/sec)<br>â€¢ 5 seconds @ 16kHz = `[80000]` values<br>â€¢ Each value = microphone displacement amplitude<br><br>**Step 2 â€” STFT:**<br>```python<br>import librosa<br>stft = librosa.stft(y, n_fft=2048, hop_length=512)<br># Output: [1025, T/hop] complex values<br># 1025 = n_fft/2 + 1 frequency bins<br>```<br><br>**Step 3 â€” Mel Scale:**<br>â€¢ Formula: `mel = 2595 Ã— logâ‚â‚€(1 + f/700)`<br>â€¢ Compresses high frequencies (perceptually uniform)<br>â€¢ 10kHz vs 10.1kHz sounds same; 100Hz vs 200Hz very different<br>```python<br>mel_spec = librosa.feature.melspectrogram(<br>    y=y, sr=16000, n_mels=128, hop_length=512<br>)<br># Output: [128, T/hop]<br>```<br><br>**Step 4 â€” Log Amplitude:**<br>â€¢ `mel_db = librosa.power_to_db(mel_spec)`<br>â€¢ Converts to decibels (human loudness perception is log)<br><br>**Result:** `[1, 128, T]` â€” treat as grayscale image for CNN/ViT | Lab 01b: Convert audio â†’ mel spectrogram â†’ visualize with `plt.imshow`. This proves audio AI is just image AI on spectrograms. Same augmentations work (crop, mask, mixup). |
| **Early Fusion** | Combine raw/preprocessed features at input level before the main model. **Deep dive â†’** Concatenate modality tensors channel-wise, feed to single encoder. Requires aligned inputs (same resolution, coordinate frame). | **Architecture:**<br>```<br>[RGB Image]    [3, 224, 224]<br>      â†“              â†“<br>[Depth Map]    [1, 224, 224]<br>      â†“              â†“<br>â”€â”€â”€â”€â”€ Concatenate â”€â”€â”€â”€â”€<br>      â†“<br>[Combined]     [4, 224, 224]<br>      â†“<br>CNN with 4 input channels<br>      â†“<br>   Prediction<br>```<br><br>**Implementation:**<br>```python<br># Modify first conv layer<br>model.conv1 = nn.Conv2d(<br>    in_channels=4,  # Was 3 for RGB<br>    out_channels=64,<br>    kernel_size=7, stride=2, padding=3<br>)<br><br># Forward pass<br>x = torch.cat([rgb, depth], dim=1)  # [B,4,H,W]<br>out = model(x)<br>```<br><br>**Pros:**<br>â€¢ Model learns cross-modal features from scratch<br>â€¢ Can discover subtle correlations<br><br>**Cons:**<br>â€¢ Requires pixel-aligned inputs<br>â€¢ Can't use pretrained single-modality models easily<br>â€¢ If one modality is noisy, contaminates everything<br><br>**When to use:** Tightly coupled sensors (RGB-D cameras) | Lab 01a: Modify VGG first layer to accept 4 channels (RGB+Depth). Must initialize new channel weights (copy from red channel or random init). |
| **Late Fusion** | Run separate models per modality, combine predictions at decision level. **Deep dive â†’** Each modality has independent encoder. Outputs (logits/probabilities) are combined via averaging, voting, or learned weights. | **Architecture:**<br>```<br>[Image] â†’ CNN_image â†’ logits_img [1000]<br>                                        â†˜<br>                                         â†’ Weighted Avg â†’ Final Prediction<br>                                        â†—<br>[Audio] â†’ CNN_audio â†’ logits_aud [1000]<br>```<br><br>**Implementation:**<br>```python<br># Separate forward passes<br>logits_img = model_image(image)  # [B, num_classes]<br>logits_aud = model_audio(audio)  # [B, num_classes]<br><br># Late fusion options:<br># 1. Simple average<br>logits = (logits_img + logits_aud) / 2<br><br># 2. Learned weights<br>logits = w1 * logits_img + w2 * logits_aud<br><br># 3. Concatenate + MLP<br>combined = torch.cat([logits_img, logits_aud], dim=1)<br>logits = fusion_mlp(combined)<br>```<br><br>**Pros:**<br>â€¢ Use best pretrained model for each modality<br>â€¢ Robust to missing modalities (just drop that branch)<br>â€¢ Simple, modular<br><br>**Cons:**<br>â€¢ No cross-modal reasoning during feature extraction<br>â€¢ "Cat in image" and "cat in text" don't interact until end<br><br>**When to use:** Heterogeneous models, ensembling, when modalities may be missing | Lab 01a: Run separate image/depth models, average probability outputs. Compare accuracy to early fusion. |
| **LiDAR vs Camera Data** | Camera = dense 2D color, ambiguous depth. LiDAR = sparse 3D structure, no texture. **Deep dive â†’** Understanding projection between coordinate systems is essential for sensor fusion in autonomous vehicles and robotics. | **Data Format Comparison:**<br><br>| Aspect | Camera | LiDAR |<br>|--------|--------|-------|<br>| Output | Dense image [3,H,W] | Sparse point cloud [N,3+] |<br>| Depth | None (2D projection) | Precise (time-of-flight) |<br>| Color | Full RGB | Intensity only |<br>| Density | Every pixel | 100k-1M points, gaps |<br>| Cost | $50-500 | $1k-75k |<br>| Weather | Fails in dark | Works at night |<br><br>**LiDAR Coordinate System:**<br>â€¢ X: forward (vehicle direction)<br>â€¢ Y: left<br>â€¢ Z: up<br>â€¢ Point: `[x, y, z, intensity]` in meters<br><br>**Camera Coordinate System:**<br>â€¢ X: right (image columns)<br>â€¢ Y: down (image rows)<br>â€¢ Z: forward (depth)<br>â€¢ Pixel: `[u, v]` in pixel units | Lab 01b: Visualize point clouds with Open3D, project onto images. Understand why LiDAR appears "sparse" when projected â€” fewer points than pixels. |
| **LiDAR â†’ Camera Projection** | Project 3D LiDAR points onto 2D camera image plane using calibration matrices. **Deep dive â†’** Two-step transformation: extrinsic (sensor poses) then intrinsic (camera optics). | **Step 1 â€” Extrinsic Transformation:**<br>Transform from LiDAR frame to camera frame:<br>```<br>P_camera = R Ã— P_lidar + t<br><br>Where:<br>â€¢ R: [3Ã—3] rotation matrix (sensor angle offset)<br>â€¢ t: [3Ã—1] translation vector (physical mounting offset)<br>â€¢ P_lidar: [3Ã—1] point in LiDAR coordinates<br>```<br><br>**Step 2 â€” Intrinsic Projection:**<br>Project 3D camera coordinates to 2D pixels:<br>```<br>â”Œ u â”       â”Œ fx  0  cx â”   â”Œ X â”<br>â”‚ v â”‚ = 1/Z â”‚ 0  fy  cy â”‚ Ã— â”‚ Y â”‚<br>â”” 1 â”˜       â”” 0   0   1 â”˜   â”” Z â”‚<br><br>Where:<br>â€¢ fx, fy: focal lengths in pixels<br>â€¢ cx, cy: principal point (optical axis intersection)<br>â€¢ Z: depth â€” division by Z creates perspective<br>```<br><br>**Complete Code:**<br>```python<br>def project_lidar_to_camera(points, R, t, K):<br>    # points: [N, 3] in LiDAR frame<br>    # Step 1: Transform to camera frame<br>    P_cam = (R @ points.T + t).T  # [N, 3]<br>    <br>    # Filter points behind camera<br>    valid = P_cam[:, 2] > 0<br>    P_cam = P_cam[valid]<br>    <br>    # Step 2: Project to pixels<br>    P_norm = P_cam / P_cam[:, 2:3]  # [N, 3]<br>    pixels = (K @ P_norm.T).T[:, :2]  # [N, 2]<br>    <br>    return pixels, P_cam[:, 2]  # pixels and depths<br>```<br><br>**Why division by Z?**<br>This is perspective projection â€” objects appear smaller when far away. Same math Renaissance painters discovered. | Lab 01b: Apply projection formula. See that LiDAR points don't cover every pixel â€” this is "sparsity." Calibration files provide R, t, K matrices. |
| **Color Mode of Image** | Channel ordering: RGB (PyTorch/PIL) vs BGR (OpenCV). **Deep dive â†’** Feeding BGR to RGB-trained model swaps redâ†”blue causing catastrophic accuracy drop. Critical preprocessing step. | **Modes:**<br>â€¢ **RGB:** Red-Green-Blue (PIL, PyTorch, most models)<br>â€¢ **BGR:** Blue-Green-Red (OpenCV default `cv2.imread()`)<br>â€¢ **RGBA:** RGB + Alpha (transparency)<br>â€¢ **Grayscale:** Single intensity channel<br><br>**Common Bug:**<br>```python<br># WRONG â€” OpenCV loads BGR<br>img = cv2.imread("photo.jpg")  # [H,W,3] in BGR<br>tensor = transforms.ToTensor()(img)  # Assumes RGB!<br><br># CORRECT â€” Convert first<br>img = cv2.imread("photo.jpg")<br>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br>tensor = transforms.ToTensor()(img)<br><br># OR â€” Use PIL directly<br>from PIL import Image<br>img = Image.open("photo.jpg")  # Already RGB<br>tensor = transforms.ToTensor()(img)<br>```<br><br>**Visual effect of BGRâ†’RGB swap:**<br>â€¢ Red apples appear blue<br>â€¢ Blue sky appears orange/red<br>â€¢ Model accuracy drops 50%+<br><br>**RGBA handling:**<br>```python<br># Most models expect 3 channels<br>img = Image.open("logo.png")  # [H,W,4] RGBA<br>img = img.convert("RGB")  # Composites alpha onto white<br>``` | Lab 01b: Check image shapes and channel ordering. Use `cv2.cvtColor()` for conversion. Always verify first pixel values match expected color. |
| **Shape of CT Scans** | 3D volumetric data: [D, H, W] where D=depth (spatial slices through body), not time. **Deep dive â†’** Requires Conv3d or slice-wise processing. Hounsfield Units (HU) encode tissue density. | **Structure:**<br>```<br>Shape: [Batch, Channels, Depth, Height, Width]<br>       [B, 1, D, H, W]<br><br>Typical: [1, 1, 200, 512, 512]<br>         â””â”€â”€ 200 slices, each 512Ã—512<br><br>D = number of axial slices (spatial depth)<br>NOT video frames â€” adjacent slices show adjacent anatomy<br>```<br><br>**Hounsfield Units (HU):**<br>```<br>Air:      -1000 HU<br>Lung:     -500 HU<br>Water:       0 HU<br>Soft tissue: +40 HU<br>Bone:     +1000 HU<br>Metal:    +3000 HU<br>```<br><br>**Windowing (visualization):**<br>```python<br># Map HU range to display range [0, 255]<br>def apply_window(volume, center, width):<br>    low = center - width // 2<br>    high = center + width // 2<br>    return np.clip((volume - low) / (high - low), 0, 1)<br><br># Lung window: center=-600, width=1500<br># Bone window: center=400, width=1800<br># Same CT, different windows reveal different anatomy<br>```<br><br>**Processing options:**<br>â€¢ `Conv3d`: Full volumetric convolution (memory heavy)<br>â€¢ Slice-wise `Conv2d` + aggregation (lighter)<br>â€¢ 2.5D: Stack adjacent slices as channels | Concept check in 01b. Key insight: You can't use standard Conv2d on raw CT â€” depth dimension contains spatial info (tumor context spans slices). |
| **Intermediate Fusion** | Modern best practice: separate encoders per modality, fusion layer at representation level. **Deep dive â†’** Cross-attention allows each modality to query the other for relevant features. Best performance, complex architecture. | **Architecture:**<br>```<br>[Image] â†’ ViT â†’ Features [196, 768]<br>                    â†“<br>              Cross-Attention<br>                    â†‘<br>[Text]  â†’ BERT â†’ Features [N, 768]<br>                    â†“<br>              Fused Features<br>                    â†“<br>               Classifier<br>```<br><br>**Cross-Attention Mechanism:**<br>```python<br># Text queries image features<br>Q = text_features @ W_q  # Query from text<br>K = image_features @ W_k  # Key from image<br>V = image_features @ W_v  # Value from image<br><br>attention = softmax(Q @ K.T / sqrt(d)) @ V<br># Output: text enriched with relevant image info<br>```<br><br>**Formula:**<br>```<br>Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V<br><br>Where:<br>â€¢ Q: queries from one modality [N, d]<br>â€¢ K, V: keys/values from other modality [M, d]<br>â€¢ âˆšd_k: scaling factor (prevents softmax saturation)<br>```<br><br>**Fusion mechanisms:**<br>â€¢ Concatenation + MLP<br>â€¢ Gated fusion (learnable weights)<br>â€¢ Multi-head cross-attention (best)<br>â€¢ Transformer encoder over concatenated tokens<br><br>**Why it wins:**<br>â€¢ Avoids early-noise issues (each encoder pretrained)<br>â€¢ Enables deep cross-modal interactions<br>â€¢ Attention weights are interpretable | Lab 02a: Implement fusion network. Concatenate features from both encoders, pass through MLP. Compare to early/late â€” intermediate typically wins. |

---

# Part 2 â€” Intermediate Fusion & Contrastive Pretraining

| Task | Details | Deep Dive: Architecture, Math, Code & Theory | Notebook / Lab Context |
|------|---------|---------------------------------------------|------------------------|
| **Contrastive Training** | Learn embeddings by pulling matched pairs together, pushing mismatched pairs apart. **Deep dive â†’** Self-supervised approach using batch structure for labels. No manual annotation needed â€” image-text pairing IS the supervision. | **The Setup:**<br>```<br>Batch of N image-text pairs:<br>(image_0, text_0) â† match<br>(image_1, text_1) â† match<br>...<br>(image_{N-1}, text_{N-1}) â† match<br><br>Positive pairs: (img_i, txt_i) for all i<br>Negative pairs: (img_i, txt_j) for all iâ‰ j<br><br>With N=1000:<br>â€¢ 1000 positive pairs<br>â€¢ 999,000 negative pairs<br>```<br><br>**Why this works:**<br>â€¢ Model must distinguish 1 correct caption from 999 wrong ones<br>â€¢ Forces learning of fine-grained visual-semantic features<br>â€¢ Larger batch = harder task = better representations<br><br>**CLIP used batch size 32,768**<br>â€¢ 32K positives vs 32KÃ—32K negatives<br>â€¢ This is why contrastive learning needs massive compute | Lab 02b: Build the contrastive training loop. Key insight: labels are implicit in batch structure â€” no human annotation required. |
| **Cosine Similarity** | Measures angle between vectors, ignoring magnitude. **Deep dive â†’** Normalized dot product. Range [-1, 1]. Standard for comparing embeddings because magnitude shouldn't affect semantic similarity. | **Formula:**<br>```<br>cosine(A, B) = (A Â· B) / (||A|| Ã— ||B||)<br>             = Î£(A_i Ã— B_i) / (âˆšÎ£(A_iÂ²) Ã— âˆšÎ£(B_iÂ²))<br><br>Range: [-1, 1]<br>â€¢ +1: Identical direction (same meaning)<br>â€¢  0: Orthogonal (unrelated)<br>â€¢ -1: Opposite directions<br>```<br><br>**vs Dot Product:**<br>```<br>dot(A, B) = Î£(A_i Ã— B_i) = ||A|| Ã— ||B|| Ã— cos(Î¸)<br><br>Problem: Magnitude conflated with direction<br>â€¢ Long vector + short vector = low dot product<br>â€¢ Even if semantically similar!<br>```<br><br>**In Practice â€” Normalize First:**<br>```python<br># After normalization, dot product = cosine similarity<br>A_norm = A / A.norm(dim=-1, keepdim=True)<br>B_norm = B / B.norm(dim=-1, keepdim=True)<br><br>similarity = A_norm @ B_norm.T  # This IS cosine sim<br><br># Equivalent to:<br>similarity = F.cosine_similarity(A, B)<br>```<br><br>**Why CLIP uses cosine:**<br>â€¢ Embedding magnitude is arbitrary<br>â€¢ Only direction encodes semantic meaning<br>â€¢ Normalized = focus on what matters | Used everywhere: contrastive loss, retrieval, CLIP inference, vector DB search. Always normalize embeddings before similarity. |
| **Dot Product** | Sum of element-wise products. **Deep dive â†’** Unbounded, affected by magnitude. Faster than cosine when vectors pre-normalized. | **Formula:**<br>```<br>dot(A, B) = Î£(A_i Ã— B_i)<br>          = A^T Ã— B  (matrix form)<br><br>For vectors of length 512:<br>dot = a_1Ã—b_1 + a_2Ã—b_2 + ... + a_512Ã—b_512<br>```<br><br>**Relationship to Cosine:**<br>```<br>dot(A, B) = ||A|| Ã— ||B|| Ã— cos(Î¸)<br><br>If ||A|| = ||B|| = 1 (unit vectors):<br>dot(A, B) = cos(Î¸)<br>```<br><br>**Matrix multiplication for batch similarity:**<br>```python<br># A: [N, D] â€” N vectors of dimension D<br># B: [M, D] â€” M vectors of dimension D<br><br>similarity_matrix = A @ B.T  # [N, M]<br># similarity_matrix[i,j] = dot(A[i], B[j])<br><br># This computes all NÃ—M pairwise similarities<br># in ONE operation â€” very efficient<br>```<br><br>**When to use each:**<br>â€¢ Dot product: When vectors pre-normalized (faster)<br>â€¢ Cosine: When magnitudes vary (safer) | Contrastive loss computation. Matrix multiply gives all pairwise similarities in one operation. |
| **CLIP Architecture** | Dual-encoder model: separate image and text encoders projecting to shared 512-dim space. **Deep dive â†’** NO cross-attention between modalities during encoding. Fusion happens only at similarity scoring. This enables offline embedding precomputation. | **Architecture Diagram:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚                    CLIP                      â”‚<br>â”‚                                               â”‚<br>â”‚  [Image]                    [Text]            â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  ViT-L/14                  Transformer        â”‚<br>â”‚  (24 layers)               (12 layers)        â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  [CLS] token               [EOS] token        â”‚<br>â”‚  [1, 1024]                 [1, 512]           â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  Linear Proj               Linear Proj        â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  [1, 512]                  [1, 512]           â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  L2 Normalize              L2 Normalize       â”‚<br>â”‚     â†“                          â†“              â”‚<br>â”‚  img_emb â”€â”€â”€â”€â”€â”€â”€ dot product â”€â”€â”€â”€â”€â”€ txt_emb  â”‚<br>â”‚                      â†“                        â”‚<br>â”‚               Similarity Score                â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**Key Insight â€” NOT Intermediate Fusion:**<br>```<br>CLIP has NO cross-attention during encoding<br>â€¢ Image encoder never sees text<br>â€¢ Text encoder never sees image<br>â€¢ They only interact via dot product at the end<br><br>This is "late fusion in embedding space"<br>```<br><br>**Why this design?**<br>```<br>Benefit: Offline precomputation<br>â€¢ Encode all images once, store embeddings<br>â€¢ New text query? Just encode query, search stored embeddings<br>â€¢ No need to re-run image encoder per query<br><br>If cross-attention existed:<br>â€¢ Would need to run both encoders together<br>â€¢ Can't precompute â€” O(NÃ—M) forward passes<br>```<br><br>**Training Data:**<br>â€¢ 400 million image-text pairs<br>â€¢ Scraped from internet (alt text, captions)<br>â€¢ WebImageText dataset | Slides reference CLIP throughout. Lab 02b loads `openai/clip-vit-base-patch32`. Understand that CLIP's power comes from scale (400M pairs) not architecture complexity. |
| **Vision Encoder Options** | ViT (Vision Transformer) or ResNet variants. **Deep dive â†’** ViT-L/14 is standard: splits image into 14Ã—14 patches, processes as sequence with transformer. | **ViT-L/14 Details:**<br>```<br>Input: [3, 224, 224] image<br><br>Step 1 â€” Patchify:<br>â€¢ 14Ã—14 = 196 patches<br>â€¢ Each patch: 224/14 = 16Ã—16 pixels<br>â€¢ Flatten: 16Ã—16Ã—3 = 768 dims per patch<br><br>Step 2 â€” Linear Embedding:<br>â€¢ Project 768 â†’ 1024 (hidden dim)<br>â€¢ Add learnable position embeddings<br>â€¢ Prepend [CLS] token<br>â€¢ Input to transformer: [197, 1024]<br><br>Step 3 â€” Transformer:<br>â€¢ 24 layers, 16 heads<br>â€¢ Output: [197, 1024]<br><br>Step 4 â€” Projection:<br>â€¢ Take [CLS] token: [1, 1024]<br>â€¢ Project to shared space: [1, 512]<br>```<br><br>**ResNet Alternative (RN50):**<br>```<br>â€¢ Standard ResNet-50 backbone<br>â€¢ Replace global average pool with AttentionPool2d<br>â€¢ Output: [1, 1024] â†’ project to [1, 512]<br>â€¢ Slightly worse than ViT but faster<br>```<br><br>**Patch size trade-offs:**<br>```<br>ViT-B/32: 32Ã—32 patches â†’ 49 patches per image (fast)<br>ViT-B/16: 16Ã—16 patches â†’ 196 patches (balanced)<br>ViT-L/14: 14Ã—14 patches â†’ 256 patches (best quality)<br><br>More patches = finer detail = more compute<br>``` | CLIP uses ViT by default. Understand patch size affects resolution/compute trade-off. |
| **Text Encoder** | Transformer with causal masking (GPT-style). **Deep dive â†’** Processes tokenized text, outputs [EOS] token embedding as sequence representation. | **Architecture:**<br>```<br>â€¢ 12 transformer layers<br>â€¢ 8 attention heads<br>â€¢ 512 hidden dimension<br>â€¢ 49,152 BPE vocabulary<br>â€¢ Max 77 tokens<br><br>Input: "a photo of a dog"<br>       â†“<br>Tokenize: [SOT, a, photo, of, a, dog, EOT, PAD...]<br>       â†“<br>Embed: [77, 512]<br>       â†“<br>Transformer (causal masking)<br>       â†“<br>Take EOT position: [1, 512]<br>       â†“<br>Linear projection â†’ [1, 512] (shared space)<br>```<br><br>**Why causal masking?**<br>```<br>Each position only attends to previous positions<br>Consistent with how text was trained (predict next token)<br><br>Attention mask:<br>[1, 0, 0, 0]  â† token 0 sees only itself<br>[1, 1, 0, 0]  â† token 1 sees 0 and 1<br>[1, 1, 1, 0]  â† token 2 sees 0, 1, 2<br>[1, 1, 1, 1]  â† token 3 sees all<br>```<br><br>**[EOT] Token:**<br>â€¢ End-of-text token position captures full sequence meaning<br>â€¢ Similar to [CLS] in BERT but at end | Text encoder processes prompts. Prompt engineering matters: "a photo of a {class}" works better than just "{class}". |
| **Ground Truth Labels** | For contrastive training, labels = indices because image_i matches text_i by construction. **Deep dive â†’** Self-supervised â€” batch structure provides supervision without manual labels. | **The Key Insight:**<br>```<br>Batch construction:<br>(image_0, text_0)  â† Pair 0<br>(image_1, text_1)  â† Pair 1<br>...<br>(image_N, text_N)  â† Pair N<br><br>Question for image_0: "Which text matches?"<br>Answer: text_0 (index 0)<br><br>Question for image_1: "Which text matches?"<br>Answer: text_1 (index 1)<br><br>Therefore: labels = [0, 1, 2, ..., N-1]<br>         = torch.arange(N)<br>```<br><br>**Why this is elegant:**<br>```<br>â€¢ No human annotation required!<br>â€¢ The pairing structure IS the label<br>â€¢ Every batch is self-labeled<br>â€¢ Scales to 400M pairs without manual work<br>```<br><br>**Code:**<br>```python<br>batch_size = len(images)<br>labels = torch.arange(batch_size, device=device)<br><br># labels[i] = i means:<br># "The correct text for image_i is text_i"<br>``` | Lab 02b: `labels = torch.arange(batch_size)`. Common student confusion â€” clarify that labels aren't class IDs, they're pair indices. |
| **Contrastive Loss (InfoNCE)** | Cross-entropy loss over similarity matrix. **Deep dive â†’** Symmetric: imageâ†’text + textâ†’image. Temperature scaling controls distribution sharpness. | **Step-by-Step Computation:**<br>```python<br># Step 1: Encode and normalize<br>img_emb = image_encoder(images)  # [N, 512]<br>txt_emb = text_encoder(texts)     # [N, 512]<br>img_emb = F.normalize(img_emb, dim=-1)<br>txt_emb = F.normalize(txt_emb, dim=-1)<br><br># Step 2: Compute similarity matrix<br>logits = img_emb @ txt_emb.T  # [N, N]<br># logits[i,j] = similarity(image_i, text_j)<br><br># Step 3: Apply temperature (learnable)<br>logits = logits * temperature  # or / temperature<br><br># Step 4: Labels = diagonal indices<br>labels = torch.arange(N, device=device)<br><br># Step 5: Cross-entropy both directions<br>loss_i2t = F.cross_entropy(logits, labels)      # rows<br>loss_t2i = F.cross_entropy(logits.T, labels)    # cols<br>loss = (loss_i2t + loss_t2i) / 2<br>```<br><br>**What cross-entropy does here:**<br>```<br>For image_0:<br>logits[0] = [sim(i0,t0), sim(i0,t1), sim(i0,t2), ...]<br>labels[0] = 0<br><br>CE loss pushes sim(i0,t0) to be highest<br>and all sim(i0,t_jâ‰ 0) to be low<br>```<br><br>**Temperature Scaling:**<br>```<br>Ï„ (temperature) controls sharpness:<br><br>logits/Ï„ with Ï„=0.07 (CLIP default):<br>â€¢ Divides by small number â†’ larger values<br>â€¢ Sharper softmax â†’ more confident<br><br>Ï„=1.0: Softer distribution<br>Ï„=0.01: Very peaked, nearly argmax<br><br>CLIP learns Ï„ as a parameter (log-scale)<br>``` | Lab 02b: Implement exact loss function. Temperature is crucial â€” too high loses signal, too low causes training instability. |
| **`repeat_interleave` vs `repeat`** | Tensor duplication for batch alignment. **Deep dive â†’** repeat_interleave = element-wise repetition (stutter). repeat = tile entire tensor (echo). Mixing them scrambles labels. | **Visual Difference:**<br>```<br>x = torch.tensor([A, B, C])<br><br>x.repeat_interleave(3):<br>â†’ [A, A, A, B, B, B, C, C, C]<br>Think: "stutter" â€” each element repeats before moving on<br><br>x.repeat(3):<br>â†’ [A, B, C, A, B, C, A, B, C]<br>Think: "echo" â€” whole sequence repeats<br>```<br><br>**When Each Is Used:**<br>```python<br># Scenario: 4 images, each needs to match 3 texts<br>images = torch.randn(4, 512)  # [I0, I1, I2, I3]<br>texts = torch.randn(3, 512)   # [T0, T1, T2]<br><br># Want: Every image paired with every text<br># (I0,T0), (I0,T1), (I0,T2), (I1,T0), ...<br><br># Images: repeat each 3 times<br>img_exp = images.repeat_interleave(3, dim=0)  # [12, 512]<br># [I0,I0,I0, I1,I1,I1, I2,I2,I2, I3,I3,I3]<br><br># Texts: tile 4 times<br>txt_exp = texts.repeat(4, 1)  # [12, 512]<br># [T0,T1,T2, T0,T1,T2, T0,T1,T2, T0,T1,T2]<br><br># Now img_exp[i] pairs with txt_exp[i] correctly<br>```<br><br>**Common Bug:**<br>```python<br># WRONG â€” using repeat for images<br>img_exp = images.repeat(3, 1)<br># [I0,I1,I2,I3, I0,I1,I2,I3, I0,I1,I2,I3]<br># Now I0 pairs with T0, I1 pairs with T1... WRONG!<br>``` | Lab 02b: Used during batch preparation. Wrong operation = scrambled image-text pairs = model learns nothing. |
| **Vector Database** | Storage system for embeddings with fast similarity search. **Deep dive â†’** HNSW (Hierarchical Navigable Small World) enables O(log N) approximate nearest neighbor search vs O(N) brute force. | **The Problem:**<br>```<br>1 million document chunks, each [768] dims<br>Query: find 10 most similar to query vector<br><br>Brute force: 1M dot products per query<br>At 1Î¼s each: 1 second per query<br>At 1000 QPS: 1000 GPUs needed<br>```<br><br>**HNSW Solution:**<br>```<br>Build multi-layer graph during indexing:<br><br>Layer 3:  A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ B (sparse)<br>Layer 2:  A â”€â”€â”€â”€â”€â”€ C â”€â”€â”€â”€â”€â”€ B<br>Layer 1:  A â”€â”€ D â”€â”€ C â”€â”€ E â”€â”€ B<br>Layer 0:  Aâ”€Dâ”€Fâ”€Gâ”€Câ”€Hâ”€Iâ”€Eâ”€Jâ”€B (dense)<br><br>Search:<br>1. Start at Layer 3 (few nodes, coarse)<br>2. Greedy move toward query<br>3. Drop to Layer 2, continue<br>4. ... until Layer 0<br>5. Local refinement in dense neighborhood<br><br>Complexity: O(log N) vs O(N)<br>```<br><br>**Parameters:**<br>```<br>M: edges per node (more = accurate, more memory)<br>ef_construction: depth during build (more = better index)<br>ef_search: depth during query (speed vs accuracy)<br><br>Typical: M=16, ef_construction=256, ef_search=128<br>```<br><br>**Vector DB Options:**<br>â€¢ Milvus (open source, distributed)<br>â€¢ Pinecone (managed service)<br>â€¢ Weaviate (hybrid search)<br>â€¢ Qdrant (Rust, fast)<br>â€¢ pgvector (Postgres extension) | Lab 04a: Use Milvus. Insert frame embeddings, query with text. DB handles similarity search internally. |
| **Retrieval Augmented Generation (RAG)** | LLM retrieves external context before generating answer. **Deep dive â†’** Reduces hallucination by grounding responses in retrieved documents. Enables fresh/private knowledge without retraining. | **Pipeline Diagram:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INDEXING (Offline) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚                                             â”‚<br>â”‚ Documents â†’ Chunk â†’ Embed â†’ Vector DB       â”‚<br>â”‚    â†“         â†“       â†“         â†“           â”‚<br>â”‚  PDFs    500 chars  BGE     Milvus          â”‚<br>â”‚  HTML    per chunk  model   stored          â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br><br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RETRIEVAL (Online) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚                                             â”‚<br>â”‚ Query â†’ Embed â†’ Vector Search â†’ Top-K Chunks â”‚<br>â”‚   â†“       â†“          â†“             â†“        â”‚<br>â”‚ "What   [768]     HNSW ANN     [chunk1,      â”‚<br>â”‚  is X?" vector    search       chunk2,       â”‚<br>â”‚                                chunk3]       â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br><br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GENERATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚                                             â”‚<br>â”‚ Augmented Prompt â†’ LLM â†’ Grounded Answer    â”‚<br>â”‚        â†“            â†“          â†“            â”‚<br>â”‚ "Context:        GPT-4     Based on         â”‚<br>â”‚  {chunks}        LLaMA     retrieved        â”‚<br>â”‚  Question:                 context...       â”‚<br>â”‚  {query}"                                   â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**Why RAG Helps:**<br>```<br>Without RAG:<br>â€¢ LLM only knows training data (cutoff date)<br>â€¢ Hallucinates when uncertain<br>â€¢ Can't cite sources<br>â€¢ No private/enterprise knowledge<br><br>With RAG:<br>â€¢ Fresh knowledge from your documents<br>â€¢ Grounded in retrieved context<br>â€¢ Can cite specific chunks<br>â€¢ Works on proprietary data<br>```<br><br>**Prompt Template:**<br>```<br>System: Answer based only on the context below.<br><br>Context:<br>{chunk_1}<br>{chunk_2}<br>{chunk_3}<br><br>Question: {user_query}<br><br>Answer:<br>``` | Lab 03b: Build RAG pipeline. Extract PDF text, chunk, embed, store, retrieve, augment prompt. Core pattern for document AI. |

---

# Part 3 â€” Cross-Modal Projection, VLMs, LLaVA & OCR Pipelines

| Task | Details | Deep Dive: Architecture, Math, Code & Theory | Notebook / Lab Context |
|------|---------|---------------------------------------------|------------------------|
| **Vision Language Models (VLMs)** | Models accepting both image + text, generating text with visual grounding. **Deep dive â†’** Core insight: treat projected image features as "visual tokens" that LLM processes alongside text tokens. | **High-Level Architecture:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚              VLM Structure               â”‚<br>â”‚                                          â”‚<br>â”‚  [Image] â†’ Vision Encoder â†’ Projection   â”‚<br>â”‚               (CLIP ViT)     (MLP)       â”‚<br>â”‚                   â†“            â†“         â”‚<br>â”‚              [257, 1024] â†’ [256, 4096]   â”‚<br>â”‚                              â†“           â”‚<br>â”‚                        Visual Tokens     â”‚<br>â”‚                              â†“           â”‚<br>â”‚  [Text] â†’ Tokenize â†’ Embed â†’ Text Tokens â”‚<br>â”‚                              â†“           â”‚<br>â”‚            [Visual Tokens] + [Text Tokens]â”‚<br>â”‚                              â†“           â”‚<br>â”‚                    LLM (LLaMA/Vicuna)    â”‚<br>â”‚                              â†“           â”‚<br>â”‚                   Generated Response      â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**The Key Insight:**<br>```<br>LLMs process token sequences.<br>Images aren't tokens.<br><br>Solution: Make image features "look like" tokens<br>â€¢ Project to same dimension as LLM embeddings<br>â€¢ LLM treats them as "foreign language" tokens<br>â€¢ Attention over [image tokens, text tokens] jointly<br>```<br><br>**Input Sequence:**<br>```<br>[IMG_1, IMG_2, ..., IMG_256, USER, :, W, h, a, t, ...]<br>â”‚â†â”€â”€ 256 visual tokens â”€â”€â†’â”‚â†â”€â”€ text tokens â”€â”€â†’â”‚<br>``` | Lab 03a: Build VLM components. Understand that VLM = Vision Encoder + Projection + LLM glued together. |
| **Cross-Modal Projection** | Learned transformation mapping vision embeddings to LLM embedding space. **Deep dive â†’** Dimension alignment is necessary but not sufficient â€” semantic alignment requires learning. | **The Problem:**<br>```<br>Vision encoder output: [batch, 256, 1024]<br>LLM embedding space:  [batch, *, 4096]<br><br>Issue 1 â€” Dimension mismatch:<br>1024 â‰  4096<br><br>Issue 2 â€” Semantic mismatch:<br>Vision features encode visual patterns<br>LLM expects "word-like" meanings<br>Feature distributions completely different<br>```<br><br>**Solution â€” Learned Projection:**<br>```python<br>class ProjectionLayer(nn.Module):<br>    def __init__(self, vision_dim=1024, llm_dim=4096):<br>        super().__init__()<br>        # Simple linear (works but limited)<br>        # self.proj = nn.Linear(vision_dim, llm_dim)<br>        <br>        # 2-layer MLP (LLaVA uses this)<br>        self.proj = nn.Sequential(<br>            nn.Linear(vision_dim, llm_dim),<br>            nn.GELU(),<br>            nn.Linear(llm_dim, llm_dim)<br>        )<br>    <br>    def forward(self, vision_features):<br>        # [B, num_patches, vision_dim] â†’ [B, num_patches, llm_dim]<br>        return self.proj(vision_features)<br>```<br><br>**Why MLP > Linear:**<br>```<br>Linear transformation: y = Wx + b<br>â€¢ Can only rotate, scale, shear, translate<br>â€¢ Preserves straight lines and ratios<br><br>But vision and language spaces have different topology:<br>â€¢ "Dog" and "cat" close in language space<br>â€¢ Dog and cat IMAGES might be far in vision space<br><br>MLP with nonlinearity can WARP the space:<br>â€¢ Bend, fold, stretch<br>â€¢ Map different topologies onto each other<br>```<br><br>**Training the Projector:**<br>```<br>Loss: MSE or contrastive between:<br>â€¢ Projected image features<br>â€¢ Target text embeddings<br><br>Goal: proj(vision_feature_of_dog) â‰ˆ llm_embedding("dog")<br>``` | Lab 03a: Train projection layer. Minimize distance between projected visual features and corresponding text embeddings. |
| **LLaVA Architecture** | Reference VLM: CLIP vision + MLP projection + Vicuna LLM. **Deep dive â†’** Two-stage training separates feature alignment (projector only) from instruction tuning (full stack). | **Complete Architecture:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚               LLaVA                      â”‚<br>â”‚                                          â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚<br>â”‚  â”‚ Vision Encoder  â”‚ â† CLIP ViT-L/14     â”‚<br>â”‚  â”‚    (FROZEN)     â”‚    pretrained       â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚<br>â”‚           â”‚ [257, 1024]                  â”‚<br>â”‚           â–¼                              â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚<br>â”‚  â”‚   Projection    â”‚ â† 2-layer MLP       â”‚<br>â”‚  â”‚   (TRAINED)     â”‚    768K params      â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚<br>â”‚           â”‚ [256, 4096]                  â”‚<br>â”‚           â–¼                              â”‚<br>â”‚      Visual Tokens                       â”‚<br>â”‚           +                              â”‚<br>â”‚      Text Tokens â† from user prompt      â”‚<br>â”‚           â”‚                              â”‚<br>â”‚           â–¼                              â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚<br>â”‚  â”‚      LLM        â”‚ â† Vicuna-7B/13B     â”‚<br>â”‚  â”‚ (FROZENâ†’LoRA)   â”‚    (LLaMA-tuned)    â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚<br>â”‚           â”‚                              â”‚<br>â”‚           â–¼                              â”‚<br>â”‚   Generated Response                     â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**Two-Stage Training:**<br>```<br>Stage 1: Feature Alignment (Pretraining)<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>Vision encoder: FROZEN<br>Projection:     TRAINED â† only this<br>LLM:            FROZEN<br><br>Data: 558K image-caption pairs (CC3M filtered)<br>Goal: Learn to "speak" LLM's language<br>Compute: ~4 hours on 8 A100s<br><br>Stage 2: Visual Instruction Tuning<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>Vision encoder: FROZEN<br>Projection:     FINE-TUNED<br>LLM:            FINE-TUNED (or LoRA)<br><br>Data: 158K instruction-following conversations<br>      Generated by GPT-4 from images<br>Goal: Learn to follow visual instructions<br>```<br><br>**Why Two Stages?**<br>```<br>Stage 1 alone: Can describe images<br>              But doesn't follow instructions well<br><br>Stage 2 alone: Expensive, unstable<br>              Projector not initialized<br><br>Both stages: Projector learns basics first<br>            Then full system fine-tuned<br>            Efficient + stable + high quality<br>``` | Slides detail LLaVA. Lab 03a replicates the architecture at smaller scale. Key insight: frozen pretrained components + small trainable connector = efficient VLM. |
| **PDF Document Chunking** | Breaking documents into retrieval-friendly pieces. **Deep dive â†’** Chunking strategy dramatically affects RAG quality. Too big = diluted relevance. Too small = lost context. | **Chunking Strategies:**<br><br>**1. Fixed-Size Chunking:**<br>```python<br>def fixed_chunk(text, size=1000, overlap=200):<br>    chunks = []<br>    for i in range(0, len(text), size - overlap):<br>        chunks.append(text[i:i+size])<br>    return chunks<br><br># Problem: May split mid-sentence, mid-table<br># "The CEO stated that profitability" | "will increase by 40%"<br>```<br><br>**2. Recursive/Hierarchical:**<br>```python<br>from langchain.text_splitter import RecursiveCharacterTextSplitter<br><br>splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=1000,<br>    chunk_overlap=200,<br>    separators=["\\n\\n", "\\n", ". ", " ", ""]<br>)<br>chunks = splitter.split_text(document)<br><br># Tries paragraph â†’ sentence â†’ word â†’ char<br># Uses largest unit that fits size limit<br>```<br><br>**3. Semantic Chunking:**<br>```python<br># Embed each sentence, find natural breakpoints<br>from sentence_transformers import SentenceTransformer<br><br>model = SentenceTransformer('all-MiniLM-L6-v2')<br>sentences = text.split('. ')<br>embeddings = model.encode(sentences)<br><br># Compute similarity between adjacent sentences<br>similarities = [cosine(emb[i], emb[i+1]) <br>                for i in range(len(emb)-1)]<br><br># Split where similarity drops (topic change)<br>breakpoints = [i for i, sim in enumerate(similarities) <br>               if sim < threshold]<br>```<br><br>**4. Layout-Aware (Structured Documents):**<br>```python<br>from unstructured.partition.pdf import partition_pdf<br><br>elements = partition_pdf("doc.pdf")<br><br># Keep structure intact:<br># - Tables as complete units<br># - Headers + following paragraphs<br># - Lists as complete items<br># - Figures + captions together<br>```<br><br>**Chunk Size Guidelines:**<br>```<br>Embedding model context: 512 tokens typical<br>LLM context for RAG: 2048-8192 tokens<br><br>Sweet spot: 200-500 tokens per chunk<br>â€¢ Large enough for context<br>â€¢ Small enough for focused retrieval<br>â€¢ With 100-token overlap<br>``` | Lab 03b: Use `unstructured` library. Compare chunking strategies. Layout-aware >> fixed-size for structured docs. |
| **Identifying Page Elements** | Detect and classify document regions: text, tables, figures, headers. **Deep dive â†’** Layout analysis + OCR. Object detection models (YOLOX, Detectron2) trained on document layouts. | **Element Types:**<br>```<br>â€¢ Title           â€” Document/section headers<br>â€¢ NarrativeText   â€” Body paragraphs<br>â€¢ ListItem         â€” Bulleted/numbered items<br>â€¢ Table            â€” Structured tabular data<br>â€¢ Figure           â€” Images, charts, diagrams<br>â€¢ Caption          â€” Figure/table descriptions<br>â€¢ Header/Footer    â€” Page metadata<br>â€¢ PageNumber       â€” Pagination<br>â€¢ Formula          â€” Mathematical equations<br>```<br><br>**Detection Pipeline:**<br>```<br>PDF/Image<br>    â†“<br>Layout Model (YOLOX/LayoutLM)<br>    â†“<br>Bounding boxes + element types<br>    â†“<br>OCR per region (Tesseract/DocTR)<br>    â†“<br>Structured output with positions<br>```<br><br>**Using Unstructured Library:**<br>```python<br>from unstructured.partition.pdf import partition_pdf<br><br>elements = partition_pdf(<br>    "document.pdf",<br>    strategy="hi_res",  # Uses vision model<br>    infer_table_structure=True  # Extract table cells<br>)<br><br>for element in elements:<br>    print(f"Type: {type(element).__name__}")<br>    print(f"Text: {element.text[:100]}...")<br>    <br># Output:<br># Type: Title<br># Text: Annual Report 2024...<br># Type: NarrativeText  <br># Text: Our company achieved record growth...<br># Type: Table<br># Text: |Revenue|Growth|....<br>```<br><br>**Table Handling:**<br>```python<br># Tables need special treatment for RAG<br>tables = [e for e in elements if type(e).__name__ == 'Table']<br><br>for table in tables:<br>    # Option 1: Keep as markdown<br>    md = table.metadata.text_as_html<br>    <br>    # Option 2: Convert to structured format<br>    df = pd.read_html(table.metadata.text_as_html)[0]<br>    <br>    # Option 3: Describe in natural language<br>    description = f"Table showing {table.text[:50]}..."<br>```<br><br>**Tools:**<br>â€¢ `unstructured` â€” Python library, free<br>â€¢ LayoutLM/LayoutLMv3 â€” Microsoft, SOTA<br>â€¢ DocTR â€” OCR + layout<br>â€¢ NVIDIA DALI â€” GPU-accelerated | Lab 03b: `unstructured` automatically tags elements. Filter by type for specialized processing. Tables â†’ preserve structure. Text â†’ chunk normally. |
| **OCR to RAG Pipeline** | End-to-end document understanding: scan â†’ structure â†’ embed â†’ retrieve â†’ answer. **Deep dive â†’** Chain of specialized components, each step affects final quality. | **Complete Pipeline:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚           OCR â†’ RAG Pipeline                 â”‚<br>â”‚                                              â”‚<br>â”‚  PDF/Scanned Doc                             â”‚<br>â”‚        â†“                                     â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚ Layout Analysis  â”‚ Detect regions         â”‚<br>â”‚  â”‚ (YOLOX/LayoutLM) â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚     OCR         â”‚ Extract text per region â”‚<br>â”‚  â”‚  (Tesseract)    â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚   Structured    â”‚ Title, Para, Table...  â”‚<br>â”‚  â”‚    Elements     â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚    Chunking     â”‚ Respect structure      â”‚<br>â”‚  â”‚  (Layout-aware) â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚   Embedding     â”‚ Chunk â†’ [768] vector   â”‚<br>â”‚  â”‚   (BGE/E5)      â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚   Vector DB     â”‚ Store + index          â”‚<br>â”‚  â”‚   (Milvus)      â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚   Retrieval     â”‚ Query â†’ top-k chunks   â”‚<br>â”‚  â”‚   (HNSW ANN)    â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚<br>â”‚  â”‚   Generation    â”‚ Chunks + query â†’ LLM   â”‚<br>â”‚  â”‚   (LLaMA/GPT)   â”‚                        â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚<br>â”‚           â†“                                  â”‚<br>â”‚     Grounded Answer                          â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**Error Propagation:**<br>```<br>OCR error â†’ Wrong text â†’ Wrong embedding â†’ Wrong retrieval<br><br>Each stage can introduce errors:<br>â€¢ Poor scan quality â†’ OCR fails<br>â€¢ Wrong layout detection â†’ Tables broken<br>â€¢ Bad chunking â†’ Context lost<br>â€¢ Weak embeddings â†’ Irrelevant retrieval<br>â€¢ Poor prompting â†’ Hallucinated answer<br><br>Quality compounds: 90%^5 stages = 59% overall<br>``` | Lab 03b: Full pipeline implementation. Debug at each stage â€” print intermediates. OCR quality critically important. |

---

# Part 4 â€” VSS, Knowledge Graphs, Vector RAG & Context-Aware RAG

| Task | Details | Deep Dive: Architecture, Math, Code & Theory | Notebook / Lab Context |
|------|---------|---------------------------------------------|------------------------|
| **VSS Applications** | Video Search & Summarization: query video archives with natural language, get timestamped results and summaries. **Deep dive â†’** Surveillance, media, sports, compliance, education â€” any domain with video at scale. | **Use Cases:**<br>```<br>Surveillance/Safety:<br>â€¢ "Show all PPE violations this week"<br>â€¢ "Find vehicles entering after hours"<br>â€¢ "Locate the red truck from yesterday"<br><br>Media Production:<br>â€¢ "Find clips of CEO mentioning revenue"<br>â€¢ "Locate all B-roll of city skylines"<br>â€¢ "Find interviews about product launch"<br><br>Sports Analytics:<br>â€¢ "Show all 3-point shots in Q4"<br>â€¢ "Find defensive formations against zone"<br>â€¢ "Locate player injuries this season"<br><br>Education:<br>â€¢ "Find lecture segments on neural networks"<br>â€¢ "Locate demonstrations of titration"<br>â€¢ "Search for explanations of derivatives"<br><br>Compliance:<br>â€¢ "Find conversations mentioning pricing"<br>â€¢ "Locate policy violation instances"<br>â€¢ "Search for unauthorized access attempts"<br>```<br><br>**Value Proposition:**<br>```<br>Without VSS:<br>â€¢ Human watches 100 hours of footage<br>â€¢ Days of work, expensive, error-prone<br><br>With VSS:<br>â€¢ Query in seconds<br>â€¢ Timestamps + summaries returned<br>â€¢ Human reviews only relevant clips<br>```<br><br>**Scale:**<br>â€¢ Security: 1000s of cameras Ã— 24/7 = petabytes<br>â€¢ Media: millions of clips, growing daily<br>â€¢ Manual review impossible | Lab 04a: Query traffic video database. Experience how VSS saves hours of manual review. |
| **NVIDIA AI Blueprint for VSS** | End-to-end reference architecture integrating CV, VLM, LLM, RAG, and databases. **Deep dive â†’** NIMs (NVIDIA Inference Microservices) for each component, orchestrated via API. | **Architecture Diagram:**<br>```<br>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>â”‚           NVIDIA AI BLUEPRINT FOR VSS                 â”‚<br>â”‚                                                        â”‚<br>â”‚  Video Files                                           â”‚<br>â”‚      â†“                                                 â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚<br>â”‚  â”‚          DATA PLANE                  â”‚              â”‚<br>â”‚  â”‚  â€¢ DeepStream SDK (decode, sample)   â”‚              â”‚<br>â”‚  â”‚  â€¢ Chunk videos (10-60s segments)    â”‚              â”‚<br>â”‚  â”‚  â€¢ Extract keyframes                 â”‚              â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚<br>â”‚                   â†“                                    â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚<br>â”‚  â”‚          CV NIMs                     â”‚              â”‚<br>â”‚  â”‚  â€¢ Object detection (people, vehicles)â”‚              â”‚<br>â”‚  â”‚  â€¢ Tracking (ID persistence)         â”‚              â”‚<br>â”‚  â”‚  â€¢ Segmentation (scene understanding)â”‚              â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚<br>â”‚                   â†“                                    â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚<br>â”‚  â”‚          VLM NIMs                    â”‚              â”‚<br>â”‚  â”‚  â€¢ Frame captioning (describe scenes)â”‚              â”‚<br>â”‚  â”‚  â€¢ CLIP embedding (visual features)  â”‚              â”‚<br>â”‚  â”‚  â€¢ OCR (text in frames)              â”‚              â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚<br>â”‚                   â†“                                    â”‚<br>â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚<br>â”‚       â†“                       â†“                        â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚<br>â”‚  â”‚ Vector DB   â”‚        â”‚ Graph DB    â”‚                â”‚<br>â”‚  â”‚ (Milvus)    â”‚        â”‚ (Neo4j)     â”‚                â”‚<br>â”‚  â”‚ Embeddings  â”‚        â”‚ Entities    â”‚                â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚<br>â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚<br>â”‚                    â†“                                   â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚<br>â”‚  â”‚     NeMo Retriever (RAG NIM)         â”‚              â”‚<br>â”‚  â”‚  â€¢ Hybrid retrieval (vector + graph) â”‚              â”‚<br>â”‚  â”‚  â€¢ Re-ranking                        â”‚              â”‚<br>â”‚  â”‚  â€¢ Context assembly                  â”‚              â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚<br>â”‚                   â†“                                    â”‚<br>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚<br>â”‚  â”‚          LLM NIM                     â”‚              â”‚<br>â”‚  â”‚  â€¢ Answer generation                 â”‚              â”‚<br>â”‚  â”‚  â€¢ Summarization                     â”‚              â”‚<br>â”‚  â”‚  â€¢ Grounded in retrieved context     â”‚              â”‚<br>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚<br>â”‚                   â†“                                    â”‚<br>â”‚         Natural Language Response                      â”‚<br>â”‚         + Timestamps + Video Clips                     â”‚<br>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>```<br><br>**NIM = NVIDIA Inference Microservice:**<br>```<br>â€¢ Containerized, optimized model serving<br>â€¢ Pre-built for common tasks<br>â€¢ GPU-accelerated inference<br>â€¢ REST API interface<br>```<br><br>**NeMo Retriever:**<br>```<br>â€¢ Ingestion: chunk + embed + index<br>â€¢ Retrieval: query + search + re-rank<br>â€¢ Supports vector, graph, hybrid<br>``` | Lab 04a: Interact with Blueprint API endpoints. Abstracts complexity of 5+ model pipeline into simple queries. |
| **VSS Architecture** | Technical implementation: video â†’ keyframes â†’ embeddings â†’ storage â†’ retrieval â†’ summarization. **Deep dive â†’** Each stage has design choices affecting latency, quality, cost. | **Processing Pipeline:**<br>```python<br># Stage 1: Video Ingestion<br>def ingest_video(video_path, chunk_duration=30):<br>    """Chunk video into segments."""<br>    chunks = []<br>    for start in range(0, video_length, chunk_duration):<br>        chunk = extract_frames(video, start, chunk_duration)<br>        chunks.append({<br>            'start': start,<br>            'end': start + chunk_duration,<br>            'frames': chunk<br>        })<br>    return chunks<br><br># Stage 2: Frame Embedding<br>def embed_chunk(chunk, clip_model):<br>    """Embed keyframes with CLIP."""<br>    keyframes = sample_keyframes(chunk['frames'], n=5)<br>    embeddings = clip_model.encode_image(keyframes)<br>    # Average or concatenate frame embeddings<br>    chunk_embedding = embeddings.mean(dim=0)<br>    return chunk_embedding<br><br># Stage 3: Caption Generation<br>def caption_chunk(chunk, vlm_model):<br>    """Generate natural language description."""<br>    keyframe = chunk['frames'][len(chunk['frames'])//2]<br>    caption = vlm_model.generate(<br>        image=keyframe,<br>        prompt="Describe this scene in detail:"<br>    )<br>    return caption<br><br># Stage 4: Indexing<br>def index_chunk(chunk_embedding, metadata, vector_db):<br>    """Store in vector database."""<br>    vector_db.insert(<br>        embedding=chunk_embedding,<br>        metadata={<br>            'video_id': metadata['video_id'],<br>            'start_time': metadata['start'],<br>            'end_time': metadata['end'],<br>            'caption': metadata['caption']<br>        }<br>    )<br><br># Stage 5: Retrieval<br>def search(query, clip_model, vector_db, top_k=10):<br>    """Semantic search over video chunks."""<br>    query_embedding = clip_model.encode_text(query)<br>    results = vector_db.search(<br>        embedding=query_embedding,<br>        top_k=top_k<br>    )<br>    return results<br><br># Stage 6: Summarization<br>def summarize(query, results, llm):<br>    """Generate natural language summary."""<br>    context = "\\n".join([r['caption'] for r in results])<br>    prompt = f"""Based on the video descriptions:<br>{context}<br><br>Answer the question: {query}"""<br>    return llm.generate(prompt)<br>```<br><br>**Design Choices:**<br>```<br>Chunk duration:<br>â€¢ 10s: Fine-grained, more storage, precise retrieval<br>â€¢ 60s: Coarse, less storage, may miss details<br>â€¢ Sweet spot: 15-30s for most applications<br><br>Keyframe sampling:<br>â€¢ 1 per chunk: Fast, may miss events<br>â€¢ 5 per chunk: Balanced<br>â€¢ Every frame: Expensive, diminishing returns<br><br>Embedding strategy:<br>â€¢ Average frames: Simple, loses temporal info<br>â€¢ Concatenate: Preserves order, larger vectors<br>â€¢ Temporal pooling: Best, complex<br>``` | Lab 04a: Implement retrieval + summarization. Chunk â†’ embed â†’ store â†’ query â†’ summarize loop. |
| **Vector RAG** | Semantic retrieval using embedding similarity. **Deep dive â†’** Find chunks with similar meaning to query, regardless of exact keyword match. Core of modern search. | **How It Works:**<br>```<br>Query: "What safety incidents occurred?"<br>          â†“<br>    Embed query â†’ [768] vector<br>          â†“<br>    Search vector DB (HNSW)<br>          â†“<br>    Return top-k similar chunks<br><br>Chunks returned:<br>â€¢ "Worker slipped near forklift" (sim: 0.82)<br>â€¢ "Near-miss with loading equipment" (sim: 0.79)<br>â€¢ "PPE violation at dock 3" (sim: 0.76)<br>```<br><br>**Advantages:**<br>```<br>â€¢ Semantic matching ("safety incidents" â†” "near-miss")<br>â€¢ No keyword engineering required<br>â€¢ Works across paraphrases and synonyms<br>â€¢ Language-agnostic with multilingual embeddings<br>```<br><br>**Limitations:**<br>```<br>â€¢ May miss exact keyword matches<br>  Query: "Show clip 47"<br>  Vector search might find similar clips, not exact<br><br>â€¢ No relational reasoning<br>  Query: "Who talked to the person in the red shirt?"<br>  Requires entity tracking, not just similarity<br><br>â€¢ No temporal reasoning<br>  Query: "What happened AFTER the alarm?"<br>  Similarity doesn't understand "after"<br>```<br><br>**Implementation:**<br>```python<br>def vector_rag(query, embedding_model, vector_db, llm):<br>    # 1. Embed query<br>    query_vec = embedding_model.encode(query)<br>    <br>    # 2. Search<br>    results = vector_db.search(query_vec, top_k=5)<br>    <br>    # 3. Build context<br>    context = "\\n\\n".join([r.text for r in results])<br>    <br>    # 4. Generate<br>    prompt = f"Context:\\n{context}\\n\\nQuestion: {query}"<br>    answer = llm.generate(prompt)<br>    <br>    return answer, results  # Return sources for citation<br>``` | Lab 04a, 04b: Standard vector search. Fast, effective for semantic queries. Foundation of RAG. |
| **Graph RAG** | Retrieval using knowledge graph relationships. **Deep dive â†’** Enables multi-hop reasoning, entity tracking, relational queries that vector search can't handle. | **Why Graphs?**<br>```<br>Query: "Which companies has the CEO of Acme worked for?"<br><br>Vector search finds:<br>â€¢ Chunks mentioning CEO<br>â€¢ Chunks mentioning Acme<br>â€¢ Chunks mentioning companies<br>BUT: Can't chain the relationships<br><br>Graph query:<br>MATCH (c:Company {name:"Acme"})<-[:CEO_OF]-(p:Person)<br>MATCH (p)-[:WORKED_AT]->(prev:Company)<br>RETURN prev.name<br><br>â†’ Follows relationship chain directly<br>```<br><br>**Knowledge Graph Structure:**<br>```<br>Nodes (Entities):<br>â€¢ Person: {name, role, ...}<br>â€¢ Company: {name, industry, ...}<br>â€¢ Location: {name, type, ...}<br>â€¢ Event: {type, timestamp, ...}<br><br>Edges (Relationships):<br>â€¢ (:Person)-[:WORKS_AT]->(:Company)<br>â€¢ (:Person)-[:LOCATED_IN]->(:Location)<br>â€¢ (:Event)-[:INVOLVES]->(:Person)<br>â€¢ (:Video)-[:SHOWS]->(:Entity)<br>```<br><br>**Building the Graph:**<br>```python<br># From video captions/analysis<br>caption = "John Smith enters warehouse at 2pm"<br><br># Extract entities<br>entities = ner_model(caption)<br># [("John Smith", "Person"), ("warehouse", "Location")]<br><br># Extract relationships<br>relations = relation_extractor(caption)<br># [("John Smith", "enters", "warehouse")]<br><br># Create graph nodes/edges<br>graph.create_node("Person", name="John Smith")<br>graph.create_node("Location", name="warehouse")<br>graph.create_edge("John Smith", "ENTERS", "warehouse",<br>                  timestamp="2pm")<br>```<br><br>**Hybrid Retrieval:**<br>```<br>1. Parse query for entities/relationships<br>2. Graph query for structured info<br>3. Vector search for semantic context<br>4. Merge results<br>5. Generate answer with full context<br>``` | Lab 04b: Use Neo4j. Visualize entity graphs. See how graph queries answer relational questions vector search can't. |
| **Context-Aware RAG** | Filtered and enriched retrieval. **Deep dive â†’** Pre-filter by metadata (time, location, source) before vector search. Post-process to add surrounding context. | **Why Context Matters:**<br>```<br>Query: "Show safety incidents from Camera 3 yesterday"<br><br>Pure vector search:<br>â€¢ Returns safety incidents from any camera, any time<br>â€¢ Irrelevant results dilute context<br><br>Context-aware search:<br>â€¢ Filter: camera=3, date=yesterday<br>â€¢ Then: vector search within filtered set<br>â€¢ Result: Precise, relevant hits<br>```<br><br>**Implementation:**<br>```python<br>def context_aware_rag(query, vector_db, llm):<br>    # 1. Extract metadata filters from query<br>    filters = extract_filters(query)  # LLM or rules<br>    # filters = {"camera": "3", "date": "2024-01-15"}<br>    <br>    # 2. Apply filters BEFORE vector search<br>    filtered_results = vector_db.search(<br>        query_embedding=embed(query),<br>        filter=filters,  # Metadata filtering<br>        top_k=10<br>    )<br>    <br>    # 3. Expand context (adjacent segments)<br>    expanded = []<br>    for result in filtered_results:<br>        prev = get_chunk(result.id - 1)  # Before<br>        next = get_chunk(result.id + 1)  # After<br>        expanded.append({<br>            'before': prev,<br>            'match': result,<br>            'after': next<br>        })<br>    <br>    # 4. Re-rank by relevance<br>    reranked = rerank_model(query, expanded)<br>    <br>    # 5. Generate with rich context<br>    context = format_context(reranked[:5])<br>    return llm.generate(f"Context:\\n{context}\\n\\nQ: {query}")<br>```<br><br>**Filter Types:**<br>```<br>Temporal:<br>â€¢ date = "2024-01-15"<br>â€¢ time_range = ["09:00", "17:00"]<br>â€¢ relative = "last 7 days"<br><br>Spatial:<br>â€¢ camera_id = "CAM_03"<br>â€¢ location = "warehouse"<br>â€¢ zone = "loading_dock"<br><br>Categorical:<br>â€¢ event_type = "safety_incident"<br>â€¢ severity = "high"<br>â€¢ status = "unresolved"<br>```<br><br>**Router Pattern:**<br>```python<br># LLM extracts structured filters from natural language<br>def extract_filters(query):<br>    prompt = f"""Extract search filters from this query:<br>    Query: {query}<br>    <br>    Return JSON with: camera, date, time, event_type<br>    If not specified, use null."""<br>    <br>    return json.loads(llm.generate(prompt))<br>``` | Lab 04a: Pass `filters` argument to search API. Restricts search scope. Critical for precision in large archives. |
| **Cypher Query Language** | Declarative graph query language for Neo4j. **Deep dive â†’** Pattern matching syntax for traversing relationships. Essential for Graph RAG. | **Basic Syntax:**<br>```cypher<br>// Nodes in parentheses<br>(n:Label {property: value})<br><br>// Relationships in brackets<br>-[r:RELATIONSHIP_TYPE]-><br><br>// Full pattern<br>MATCH (person:Person)-[r:WORKS_AT]->(company:Company)<br>WHERE company.name = "Acme"<br>RETURN person.name, r.since<br>```<br><br>**Core Commands:**<br>```cypher<br>// CREATE - Add nodes/edges<br>CREATE (p:Person {name: "John", role: "engineer"})<br><br>// MATCH - Find patterns<br>MATCH (p:Person {name: "John"})<br>RETURN p<br><br>// MERGE - Create if not exists<br>MERGE (c:Company {name: "Acme"})<br><br>// WHERE - Filter results<br>MATCH (p:Person)<br>WHERE p.age > 30<br>RETURN p<br><br>// Relationships<br>MATCH (a)-[:KNOWS]->(b)  // Directed<br>MATCH (a)-[:KNOWS]-(b)   // Either direction<br>```<br><br>**Multi-Hop Queries:**<br>```cypher<br>// 2-hop: Friends of friends<br>MATCH (p:Person {name: "Alice"})-[:KNOWS]->()-[:KNOWS]->(fof)<br>WHERE fof <> p  // Exclude self<br>RETURN DISTINCT fof.name<br><br>// Variable length: 1-3 hops<br>MATCH (start)-[:CONNECTED*1..3]-(end)<br>RETURN end<br><br>// Shortest path<br>MATCH path = shortestPath(<br>  (a:Person {name: "Alice"})-[*]-(b:Person {name: "Bob"})<br>)<br>RETURN path<br>```<br><br>**Aggregation:**<br>```cypher<br>// Count relationships<br>MATCH (p:Person)-[:WORKED_AT]->(c:Company)<br>RETURN p.name, count(c) as num_companies<br>ORDER BY num_companies DESC<br><br>// Collect into list<br>MATCH (p:Person)-[:KNOWS]->(friend)<br>RETURN p.name, collect(friend.name) as friends<br>```<br><br>**LLM-Generated Cypher (G-Retriever):**<br>```python<br>def text_to_cypher(natural_query, llm):<br>    prompt = f"""Convert to Cypher query:<br>    <br>    Schema:<br>    (:Person)-[:WORKS_AT]->(:Company)<br>    (:Person)-[:LOCATED_IN]->(:Location)<br>    <br>    Question: {natural_query}<br>    <br>    Cypher:"""<br>    <br>    return llm.generate(prompt)<br><br># "Who works at Acme?" â†’<br># MATCH (p:Person)-[:WORKS_AT]->(c:Company {name:"Acme"})<br># RETURN p.name<br>``` | Lab 04b: Execute Cypher queries against Neo4j. Understand pattern syntax. See correspondence between query and graph visualization. |

---

# Certification Checklist â€” Quick Self-Test

| Topic | Can You Explain? | Can You Code/Diagram? |
|-------|-----------------|----------------------|
| CNN architecture | Convolution, pooling, receptive field, feature hierarchy | Draw layer stack, explain kernel sliding |
| PyTorch basics | Module, forward, backward, DataLoader, device | Write training loop, move tensors to GPU |
| Vision data shapes | RGB [3,H,W], grayscale [1,H,W], CT [D,H,W], point cloud [N,3+] | Reshape tensors, convert between formats |
| Audio spectrograms | STFT, mel scale, log amplitude, why it enables vision models | `librosa.melspectrogram()` pipeline |
| Color modes | RGB vs BGR, RGBA handling, common bugs | `cv2.cvtColor()`, PIL conversion |
| LiDAR projection | Extrinsic R,t â†’ Intrinsic K â†’ perspective division | Projection formula, code implementation |
| Early/Late/Intermediate fusion | When each is used, trade-offs, CLIP's position | Architecture diagrams for each |
| Contrastive training | Positive/negative pairs, batch as supervision | Loss computation code |
| Cosine vs dot product | Formula, when to normalize, why CLIP uses cosine | Vector math, normalization code |
| CLIP architecture | Dual encoder, NO cross-attention, shared embedding space | Architecture diagram, zero-shot inference |
| Ground truth labels | Why `torch.arange(N)`, self-supervised insight | Code snippet, batch structure explanation |
| InfoNCE loss | Symmetric cross-entropy, temperature scaling | Full loss function implementation |
| `repeat_interleave` vs `repeat` | Element-wise vs tensor-wise, when to use each | Code examples, alignment visualization |
| VLM architecture | Vision encoder â†’ projection â†’ LLM | Full diagram with dimensions |
| Cross-modal projection | Why MLP, dimension + semantic alignment | Projection layer code |
| LLaVA | Two-stage training, frozen components, efficiency | Training stage diagram |
| PDF chunking | Fixed, recursive, semantic, layout-aware | Comparison table, code examples |
| Page element identification | Element types, detection pipeline, tools | `unstructured` usage |
| Vector database | HNSW, ANN, complexity, parameters | Index creation, search code |
| RAG pipeline | Index â†’ retrieve â†’ augment â†’ generate | Full pipeline diagram |
| Vector vs Graph RAG | When each excels, limitations | Query examples for each |
| Context-aware RAG | Metadata filtering, re-ranking, context expansion | Filter extraction code |
| Cypher queries | Pattern syntax, multi-hop, aggregation | Write basic queries |
| VSS applications | Use cases, value proposition | Domain examples |
| NVIDIA Blueprint | Components, NIMs, data flow | Architecture diagram |

---

*End of Master Certification Table*
